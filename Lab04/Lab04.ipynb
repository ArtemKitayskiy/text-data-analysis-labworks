{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI1RtmePmEfs",
        "outputId": "c6f3fb57-5758-4bb7-e16b-fac2d27c8553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Лабораторная работа №4. Использование нейронных сетей для генерации текста\n",
        "**Выполнил:** Китайский А.С.  \n",
        "**Проверил:** Мохов А.С."
      ],
      "metadata": {
        "id": "d0lKMgnXmIk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "DVRY46B_mKkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "VIsIp0zJoYsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSj9pdtfTALU",
        "outputId": "b61e68c1-b965-4411-e575-5678aa2cd211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Загрузка выборки стихотворений Пушкина"
      ],
      "metadata": {
        "id": "xlr_sFscrLsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poet = 'pushkin'\n",
        "\n",
        "path_to_file = f'{poet}.txt'\n",
        "path_to_file = tf.keras.utils.get_file(\n",
        "    path_to_file, \n",
        "    f'http://uit.mpei.ru/git/main/TDA/raw/branch/master/assets/poems/{path_to_file}'\n",
        ")"
      ],
      "metadata": {
        "id": "8MOqSnQ-oeU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# путь к файлу\n",
        "path_to_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "82GvU1GAp9ZZ",
        "outputId": "5b2e65b6-e280-4ca6-c025-9148a1f60679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/pushkin.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузим текст из файла\n",
        "with open(path_to_file, encoding = \"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f'Длина текста: {len(text)} символов')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTKGQs6tpAA-",
        "outputId": "fd07218d-f6e2-49ba-9fc1-ec36aea5108b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Длина текста: 586731 символов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Первые 500 символов текста:', text[:500], sep='\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpOxEtwJr3Gd",
        "outputId": "6c91779a-56db-4e1e-d484-9ad7cef04707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Первые 500 символов текста:\n",
            "\n",
            "Так и мне узнать случилось,\n",
            "Что за птица Купидон;\n",
            "Сердце страстное пленилось;\n",
            "Признаюсь – и я влюблен!\n",
            "Пролетело счастья время,\n",
            "Как, любви не зная бремя,\n",
            "Я живал да попевал,\n",
            "Как в театре и на балах,\n",
            "На гуляньях иль в воксалах\n",
            "Легким зефиром летал;\n",
            "Как, смеясь во зло Амуру,\n",
            "Я писал карикатуру\n",
            "На любезный женской пол;\n",
            "Но напрасно я смеялся,\n",
            "Наконец и сам попался,\n",
            "Сам, увы! с ума сошел.\n",
            "Смехи, вольность – всё под лавку\n",
            "Из Катонов я в отставку,\n",
            "И теперь я – Селадон!\n",
            "Миловидной жрицы Тальи\n",
            "Видел прел\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Познакомимся с данными. Проанализируем статистические характеристики исходных данных (среднюю длину стихотворения, среднюю длину строки)."
      ],
      "metadata": {
        "id": "wugk3Px3rgrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = '</s>'\n",
        "\n",
        "def mean_line_len(poem):\n",
        "    lines = [len(line.strip()) for line in poem.split('\\n') if len(line.strip()) > 0]\n",
        "    return sum(lines)/len(lines)\n",
        "\n",
        "def describe_poems(text, return_df = False):\n",
        "    '''Функция разбирает файл на отдельные стрихотворения и расчитывает их хар-ки'''\n",
        "    poems_list = [poem.strip() for poem in text.split(EOS_TOKEN) if len(poem.strip()) > 0]\n",
        "    df = pd.DataFrame(data=poems_list, columns=['poem'])\n",
        "    df['len'] = df.poem.map(len)                     # длина\n",
        "    df['lines'] = df.poem.str.count('\\n')            # количество строк\n",
        "    df['mean_line_len'] = df.poem.map(mean_line_len) # средняя длина строки\n",
        "    if return_df:\n",
        "        return df\n",
        "    return df.describe()"
      ],
      "metadata": {
        "id": "oa4SHm9VrkxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poem_df = describe_poems(text, return_df = True)\n",
        "poem_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tEenW4P_uTVL",
        "outputId": "4a55d590-ff2e-402a-884f-48d2f8af7ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  poem   len  lines  \\\n",
              "0    Так и мне узнать случилось,\\nЧто за птица Купи...  2536    109   \n",
              "1    Хочу воспеть, как дух нечистый Ада\\nОседлан бы...  5543    170   \n",
              "2    Покаместь ночь еще не удалилась,\\nПокаместь св...  4279    131   \n",
              "3    Ах, отчего мне дивная природа\\nКорреджио искус...  4435    131   \n",
              "4    Арист! и ты в толпе служителей Парнасса!\\nТы х...  3893    106   \n",
              "..                                                 ...   ...    ...   \n",
              "714  Чудный сон мне бог послал —\\n\\nС длинной белой...   860     38   \n",
              "715  О нет, мне жизнь не надоела,\\nЯ жить люблю, я ...   196      7   \n",
              "716  \"Твой и мой, – говорит Лафонтен —\\nРасторгло у...   187      5   \n",
              "717  Когда луны сияет лик двурогой\\nИ луч ее во мра...   269      7   \n",
              "718  Там, устарелый вождь! как ратник молодой,\\nИск...   256      5   \n",
              "\n",
              "     mean_line_len  \n",
              "0        23.114286  \n",
              "1        33.372671  \n",
              "2        33.451613  \n",
              "3        33.364341  \n",
              "4        38.642857  \n",
              "..             ...  \n",
              "714      22.833333  \n",
              "715      23.625000  \n",
              "716      30.333333  \n",
              "717      32.750000  \n",
              "718      41.833333  \n",
              "\n",
              "[719 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59bc79ab-4326-453c-9a3b-a870abdf72ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poem</th>\n",
              "      <th>len</th>\n",
              "      <th>lines</th>\n",
              "      <th>mean_line_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Так и мне узнать случилось,\\nЧто за птица Купи...</td>\n",
              "      <td>2536</td>\n",
              "      <td>109</td>\n",
              "      <td>23.114286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Хочу воспеть, как дух нечистый Ада\\nОседлан бы...</td>\n",
              "      <td>5543</td>\n",
              "      <td>170</td>\n",
              "      <td>33.372671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Покаместь ночь еще не удалилась,\\nПокаместь св...</td>\n",
              "      <td>4279</td>\n",
              "      <td>131</td>\n",
              "      <td>33.451613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ах, отчего мне дивная природа\\nКорреджио искус...</td>\n",
              "      <td>4435</td>\n",
              "      <td>131</td>\n",
              "      <td>33.364341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Арист! и ты в толпе служителей Парнасса!\\nТы х...</td>\n",
              "      <td>3893</td>\n",
              "      <td>106</td>\n",
              "      <td>38.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>Чудный сон мне бог послал —\\n\\nС длинной белой...</td>\n",
              "      <td>860</td>\n",
              "      <td>38</td>\n",
              "      <td>22.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>715</th>\n",
              "      <td>О нет, мне жизнь не надоела,\\nЯ жить люблю, я ...</td>\n",
              "      <td>196</td>\n",
              "      <td>7</td>\n",
              "      <td>23.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>716</th>\n",
              "      <td>\"Твой и мой, – говорит Лафонтен —\\nРасторгло у...</td>\n",
              "      <td>187</td>\n",
              "      <td>5</td>\n",
              "      <td>30.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>717</th>\n",
              "      <td>Когда луны сияет лик двурогой\\nИ луч ее во мра...</td>\n",
              "      <td>269</td>\n",
              "      <td>7</td>\n",
              "      <td>32.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>Там, устарелый вождь! как ратник молодой,\\nИск...</td>\n",
              "      <td>256</td>\n",
              "      <td>5</td>\n",
              "      <td>41.833333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>719 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59bc79ab-4326-453c-9a3b-a870abdf72ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59bc79ab-4326-453c-9a3b-a870abdf72ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59bc79ab-4326-453c-9a3b-a870abdf72ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "describe_poems(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "zp251Fv1td6P",
        "outputId": "fa45b719-fa45-4d53-ff89-e1a3c177cba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               len       lines  mean_line_len\n",
              "count   719.000000  719.000000     719.000000\n",
              "mean    808.037552   29.464534      27.445404\n",
              "std    1046.786862   39.244020       5.854564\n",
              "min      74.000000    5.000000       8.250000\n",
              "25%     280.500000    9.000000      24.125000\n",
              "50%     453.000000   16.000000      25.758065\n",
              "75%     852.000000   33.000000      31.522727\n",
              "max    8946.000000  437.000000      48.923077"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4ab69c8-3c17-474f-9d2a-561a02158057\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "      <th>lines</th>\n",
              "      <th>mean_line_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>719.000000</td>\n",
              "      <td>719.000000</td>\n",
              "      <td>719.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>808.037552</td>\n",
              "      <td>29.464534</td>\n",
              "      <td>27.445404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1046.786862</td>\n",
              "      <td>39.244020</td>\n",
              "      <td>5.854564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>74.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>8.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>280.500000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>24.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>453.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>25.758065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>852.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>31.522727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>8946.000000</td>\n",
              "      <td>437.000000</td>\n",
              "      <td>48.923077</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4ab69c8-3c17-474f-9d2a-561a02158057')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d4ab69c8-3c17-474f-9d2a-561a02158057 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d4ab69c8-3c17-474f-9d2a-561a02158057');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Подготовим выборку для обучения"
      ],
      "metadata": {
        "id": "9fQtSfnxvUK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# разобьем данные на тренировочные, валидационные и тестовые\n",
        "train_poems, test_poems = train_test_split(poem_df.poem.to_list(), test_size=0.1, random_state=RANDOM_STATE)\n",
        "train_poems, val_poems = train_test_split(train_poems, test_size=0.1, random_state=RANDOM_STATE)\n",
        "\n",
        "train_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(train_poems)\n",
        "val_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(val_poems)\n",
        "test_poems = f'\\n\\n{EOS_TOKEN}\\n\\n'.join(test_poems)"
      ],
      "metadata": {
        "id": "zcZE4A-nvXsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создадим словарь уникальных символов из текста\n",
        "vocab = sorted(set(text)) + [EOS_TOKEN]\n",
        "\n",
        "print(f'{len(vocab)} уникальных символа')\n",
        "print('Словарь:', vocab, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdd_ubSiwDqC",
        "outputId": "55942287-9b53-41d1-9ca5-3c5e302f322f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143 уникальных символа\n",
            "Словарь:\n",
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '\\xa0', '«', '»', 'à', 'â', 'ç', 'è', 'é', 'ê', 'ô', 'û', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', '–', '—', '„', '…', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Закодируем текст с помощью слоя StringLookup"
      ],
      "metadata": {
        "id": "EYSHhYZV-Sa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), \n",
        "    mask_token=None\n",
        ")\n",
        "\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True,    # сопоставление индексов с элементами словаря\n",
        "    mask_token=None\n",
        ")\n",
        "\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy().decode('utf-8')\n",
        "    \n",
        "def ids_from_text(text):\n",
        "    return ids_from_chars(tf.strings.unicode_split(text, input_encoding='UTF-8'))"
      ],
      "metadata": {
        "id": "ifkmmCVlwxkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример кодирования\n",
        "ids = ids_from_text(train_poems[:30])\n",
        "res_text = text_from_ids(ids)\n",
        "print('Исходный текст:', train_poems[:30], sep = '\\n')\n",
        "print('Закодированный текст:', ids.numpy(), sep = '\\n')\n",
        "print('Декодированный текст:', res_text, sep = '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SmrQeGJxEUc",
        "outputId": "8ae0b1b7-4a5f-43b4-cf42-eb30957daf1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст:\n",
            "Корабль испанский трехмачтовый\n",
            "Закодированный текст:\n",
            "[ 87 120 122 106 107 117 134   2 114 123 121 106 119 123 116 114 115   2\n",
            " 124 122 111 127 118 106 129 124 120 108 133 115]\n",
            "Декодированный текст:\n",
            "Корабль испанский трехмачтовый\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# кодируем данные и преобразуем их в датасеты\n",
        "train_ids = ids_from_text(train_poems)\n",
        "val_ids = ids_from_text(val_poems)\n",
        "test_ids = ids_from_text(test_poems)\n",
        "\n",
        "train_ids_dataset = tf.data.Dataset.from_tensor_slices(train_ids)\n",
        "val_ids_dataset = tf.data.Dataset.from_tensor_slices(val_ids)\n",
        "test_ids_dataset = tf.data.Dataset.from_tensor_slices(test_ids)"
      ],
      "metadata": {
        "id": "rmFXUO-BxHrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разобьем текст на последовательности длины seq_length"
      ],
      "metadata": {
        "id": "wMzWcfEQGKBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(train_ids_dataset) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "xmtevy7DzLtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# параметры: размер последовательности и дроп последней уменьшенной последовательности\n",
        "train_sequences = train_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "val_sequences = val_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "test_sequences = test_ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# одна последовательность из train\n",
        "for seq in train_sequences.take(1):\n",
        "  print(text_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHr3Gb1azOK-",
        "outputId": "bd04fbc7-f70f-4201-a521-16b4a148c917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Корабль испанский трехмачтовый,\n",
            "Пристать в Голландию готовый:\n",
            "На нем мерзавцев сотни три,\n",
            "Две обезьян\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим сдвиг target относительно input на один символ"
      ],
      "metadata": {
        "id": "NGpIuGQ2FjNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "GD-HJifsz9BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_sequences.map(split_input_target)\n",
        "val_dataset = val_sequences.map(split_input_target)\n",
        "test_dataset = test_sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "4oiqbopx0Pan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in train_dataset.take(1):\n",
        "    print(\"Input:\", text_from_ids(input_example), sep='\\n')\n",
        "    print('\\n\\n')\n",
        "    print(\"Target:\", text_from_ids(target_example), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsgOfPSD0Q3Y",
        "outputId": "ab6a28c3-a57c-4d98-b53e-a930a5e637b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "Корабль испанский трехмачтовый,\n",
            "Пристать в Голландию готовый:\n",
            "На нем мерзавцев сотни три,\n",
            "Две обезья\n",
            "\n",
            "\n",
            "\n",
            "Target:\n",
            "орабль испанский трехмачтовый,\n",
            "Пристать в Голландию готовый:\n",
            "На нем мерзавцев сотни три,\n",
            "Две обезьян\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перемешаем датасеты и разобьем их на батчи для оптимизации обучения"
      ],
      "metadata": {
        "id": "vEV0C-YkGkLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "    dataset = (\n",
        "        dataset\n",
        "        .shuffle(BUFFER_SIZE) # перемешивание данных\n",
        "        .batch(BATCH_SIZE, drop_remainder=True) # разбиение на батчи размером BATCH_SIZE\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)) # загрузка данных в память заранее\n",
        "    return dataset \n",
        "\n",
        "train_dataset = prepare_dataset(train_dataset)\n",
        "val_dataset = prepare_dataset(val_dataset)\n",
        "test_dataset = prepare_dataset(test_dataset)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsK7preM0TvI",
        "outputId": "caa2f0f3-1c7c-4c8f-fa02-43d239a903fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Построим нейронную сеть. Тип ячейки GRU"
      ],
      "metadata": {
        "id": "EBTc8wYDxqHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель состоит из трех слоев\n",
        "\n",
        "- tf.keras.layers.Embedding: Входной слой. Кодирует каждый идентификатор символа в вектор размерностью embedding_dim;\n",
        "- tf.keras.layers.GRU: Рекуррентный слой на ячейках GRU в количестве gru_units\n",
        "- tf.keras.layers.Dense: Выходной полносвязный слой размерностью vocab_size, в который выводится вероятность каждого символа в словаре."
      ],
      "metadata": {
        "id": "ZRtbs2zB0tsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# длина словаря символов\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# размерность Embedding'а\n",
        "embedding_dim = 256 #@param{type:\"number\"}\n",
        "\n",
        "# Параметры GRU-слоя\n",
        "gru_units = 300 #@param {type:\"number\"}\n",
        "dropout_p = 0.5\n",
        "\n",
        "T = 0.3 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "N = 1000"
      ],
      "metadata": {
        "id": "msItlzMG0zGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, gru_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(\n",
        "        gru_units,             # размерность выхода\n",
        "        dropout = dropout_p,\n",
        "        return_sequences=True, # возвращает всю последовательность\n",
        "        return_state=True)     # возвращает последнее состояние\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    \n",
        "    states = self.gru.get_initial_state(x)\n",
        "\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "QCrVmCPq1MaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    gru_units=gru_units)"
      ],
      "metadata": {
        "id": "TgIGfARr1Vf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Иллюстрация работы сети:\n",
        "![image](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ],
      "metadata": {
        "id": "S6Vr58TKTRn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка необученой модели"
      ],
      "metadata": {
        "id": "sJK32KmBNXCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на один батч из датасета\n",
        "for input_example_batch, target_example_batch in train_dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\"(размер батча, длина последовательности, размер словаря) -\", example_batch_predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XVq2Wpo1wU6",
        "outputId": "9b506222-40a3-4b04-a41f-caf14ad58524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(размер батча, длина последовательности, размер словаря) - (64, 100, 144)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на логиты вероятности каждого символа на первой позиции\n",
        "example_batch_predictions[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17vuR5Nb11bf",
        "outputId": "bf78c55a-23f4-4768-e778-6ca49471138f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(144,), dtype=float32, numpy=\n",
              "array([ 0.0027603 , -0.00812737,  0.00038508,  0.00031491,  0.01299553,\n",
              "       -0.00303674,  0.00711669, -0.01294534,  0.01583236, -0.00627203,\n",
              "       -0.00348256,  0.02125814, -0.00914515,  0.01366937, -0.00350473,\n",
              "       -0.00257265,  0.00183849,  0.01162545,  0.00269023,  0.0090152 ,\n",
              "       -0.01085356,  0.00805729,  0.00256377,  0.01179903, -0.00966423,\n",
              "        0.00375698, -0.00679599,  0.0154108 ,  0.00180047,  0.01141567,\n",
              "        0.0084399 ,  0.02246128, -0.02589275,  0.00823148, -0.01511441,\n",
              "        0.00904186, -0.00039597, -0.00019091,  0.00401311, -0.0088466 ,\n",
              "        0.00698363,  0.00074551, -0.00780546,  0.00096848,  0.0072231 ,\n",
              "        0.00835975, -0.00634251, -0.01854552,  0.01088605,  0.00218695,\n",
              "       -0.00554847, -0.00145263,  0.00696061,  0.00304085, -0.01234441,\n",
              "       -0.01479187,  0.02132496, -0.00581373,  0.00124562, -0.00357364,\n",
              "       -0.00045178, -0.00542335,  0.00045016, -0.00489227, -0.01009251,\n",
              "       -0.00737071, -0.01680355, -0.01748476,  0.00393472, -0.00306959,\n",
              "       -0.01292243, -0.00171492,  0.01907193, -0.00658137,  0.00870272,\n",
              "       -0.00043115, -0.00655309, -0.00233603, -0.01708768, -0.00131634,\n",
              "       -0.01095936, -0.00640995,  0.00449343,  0.0026435 ,  0.01594478,\n",
              "       -0.00404843, -0.01731994,  0.03242161,  0.0015934 ,  0.00108438,\n",
              "        0.0061175 , -0.00076222, -0.00072782,  0.00605589, -0.00822833,\n",
              "        0.01565308,  0.01593391, -0.00343861,  0.00574894,  0.00346378,\n",
              "       -0.00215358,  0.00809943,  0.00307374,  0.00525694,  0.00584146,\n",
              "       -0.01016823, -0.02381774, -0.0016217 , -0.01185727,  0.01012895,\n",
              "       -0.00403682, -0.0136862 , -0.00108103,  0.00440203,  0.03100161,\n",
              "        0.0072445 ,  0.01082066, -0.00441508,  0.02100765, -0.01100058,\n",
              "       -0.02004974,  0.00751253,  0.02336345, -0.01001837, -0.00910527,\n",
              "       -0.00286946, -0.00472749, -0.00309926, -0.00570999, -0.00580452,\n",
              "        0.00639715,  0.01078508, -0.00176614, -0.01042429, -0.00120519,\n",
              "       -0.01384079, -0.00960071,  0.00759125, -0.01770626,  0.00084064,\n",
              "       -0.00321993,  0.01060948, -0.00804819,  0.00204523], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JCZ-oSM15CO",
        "outputId": "0e06ae2e-4284-4aa9-e100-1cc6d4746c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([111,  95,  46,  38, 105, 114,  23,  70, 127,  66,  45,  75,   2,\n",
              "        58,  85,  79,   9,  92,  51,  71, 126,  19, 125,  57,  58, 107,\n",
              "       130,  50,  78,  79,   1,  89, 142,  78,  64,  19,  94, 135,  93,\n",
              "        89,   9, 109,  56,   5,  54,  79,  29,  20, 103, 112, 128,  73,\n",
              "        14, 136,  81, 132,  92, 100,  98,  69,  86,  26, 121,  24,  40,\n",
              "       129, 129,  26,  95, 123,  74,  74,  91, 100,  68,  68,  19, 126,\n",
              "        66, 129,  43, 112, 134,  90, 141, 132,  32,  59,  54,  95,  23,\n",
              "        29, 114,  48,  86, 125,  81,  93,  83,  36])"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\", text_from_ids(input_example_batch[0]), sep='\\n')\n",
        "print()\n",
        "print(\"Predictions:\", text_from_ids(sampled_indices), sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4aEDH0x19sF",
        "outputId": "1f358802-36dc-4746-8007-b7571171adca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "исходит светлый Мир.\n",
            "Свершилось!.. Русской царь, достиг ты славной цели!\n",
            "Вотще надменные на родину л\n",
            "\n",
            "Predictions:\n",
            "еТfWЯиFâх eô rИВ,ПkçфBуqrбшjБВ\n",
            "М…БyBСэРМ,гp'nВNCЭжцé;юДъПЧХàЙJпH_ччJТсêêОЧ»»Bф чcжьН„ъQsnТFNиhЙуДРЖU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройка модели для обучения"
      ],
      "metadata": {
        "id": "0JJxC1FRUtsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# функция потерь\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "LMOCFVVD1n_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# пример\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: (размер батча, длина последовательности, размер словаря) -\", example_batch_predictions.shape)\n",
        "print(\"Mean loss:       \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPZaiwTb2NXK",
        "outputId": "f0972e3c-f302-4769-eb99-a8c701201e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape: (размер батча, длина последовательности, размер словаря) - (64, 100, 144)\n",
            "Mean loss:        tf.Tensor(4.9706573, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('perplexity: ',np.exp(example_batch_mean_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gixeQTLL2R_B",
        "outputId": "c594f987-61e5-48d2-9153-6fd487b20985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  144.1216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перплексия приблизительно равна размеру словаря, что говорит о полной неопределенности модели при генерации текста."
      ],
      "metadata": {
        "id": "M89QRKoJjzih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "Xk7hjYhI2Uzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dz5qnpk2WVb",
        "outputId": "4ab8247a-8943-4668-8c8f-26e32489d4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_40 (Embedding)    multiple                  36864     \n",
            "                                                                 \n",
            " gru_27 (GRU)                multiple                  502200    \n",
            "                                                                 \n",
            " dense_40 (Dense)            multiple                  43344     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 582,408\n",
            "Trainable params: 582,408\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# используем tf.keras.callbacks.ModelCheckpoint, чтобы убедиться, \n",
        "# что контрольные точки сохраняются во время обучения:\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    monitor=\"val_loss\",\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "X-PbhdQf2aG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создадим модель, реализующую один шаг предсказания\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Создаем маску, чтобы предотвратить создание \"[UNK]\"\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # ставим знак \"-\" перед каждым неверным индексом\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    \n",
        "  # Этот фрагмент целиком написан с использованием Tensorflow, поэтому его можно выполнять \n",
        "  # не с помощью интерпретатора языка Python, а через граф операций. Это будет значительно быстрее.  \n",
        "  # Для этого воспользуемся декоратором  @tf.function   \n",
        "  @tf.function   \n",
        "  def generate_one_step(self, inputs, states=None, temperature=1.0):\n",
        "    # преобразуем строки в идентификаторы токенов.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # запускаем модель\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # используем только последнее предсказание.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/temperature\n",
        "    # применяем маску прогнозирования\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # используем выходные логиты для генерации идентификаторов токенов.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # преобразование идентификаторов токенов в символы\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "\n",
        "    # возвращаем символы и состояние модели\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "60N3bSz8l8bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Обучим нейронную сеть на разных количествах эпох (5, 15, 30, 50, 70) при зафиксированных параметрах embedding_dim = 256, gru_units = 300, T = 0.3 и сравним результаты генерации (тексты), перплексию и статистические характеристики сгенерированных текстов. Выберем оптимальное количество эпох"
      ],
      "metadata": {
        "id": "47nwnmU0vfCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = [5, 15, 30, 50, 70]"
      ],
      "metadata": {
        "id": "mBAZCO5w2bon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_epochs= pd.DataFrame(\n",
        "    columns=['eval_loss', 'perplexity', 'result_text', 'run_time'], \n",
        "    index=['Epochs 5', 'Epochs 15', 'Epochs 30', 'Epochs 50', 'Epochs 70'])\n",
        "dict_epochs_describe = {}"
      ],
      "metadata": {
        "id": "yaEP3Y_2pvXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in NUM_EPOCHS:\n",
        "  model = MyModel(\n",
        "        vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "        embedding_dim=embedding_dim,\n",
        "        gru_units=gru_units)\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "  history = model.fit(\n",
        "      train_dataset, \n",
        "      validation_data=val_dataset, \n",
        "      epochs=i, \n",
        "      callbacks=[checkpoint_callback]\n",
        "  )\n",
        "  eval_loss = model.evaluate(test_dataset)\n",
        "  perplexity = np.exp(eval_loss)\n",
        "  print('eval_loss:', eval_loss)\n",
        "  print('perplexity', perplexity)\n",
        "  \n",
        "  one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "  start = time.time()\n",
        "  states = None\n",
        "  next_char = tf.constant(['\\n'])\n",
        "  result = [next_char]\n",
        "\n",
        "  for n in range(N):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states, temperature=T)\n",
        "    result.append(next_char)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  end = time.time()\n",
        "\n",
        "  result_text = result[0].numpy().decode('utf-8')\n",
        "  print(result_text)\n",
        "  print('_'*80)\n",
        "  run_time = end - start\n",
        "  print('\\nrun time:', run_time)\n",
        "\n",
        "  df_epochs.loc[f'Epochs {i}'] = [eval_loss, perplexity, result_text, run_time]\n",
        "  dict_epochs_describe[f'Epochs {i}'] = describe_poems(result_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1sSl_1w2fX_",
        "outputId": "d53f7096-b448-42c9-f211-bb5438c2ac17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "72/72 [==============================] - 25s 70ms/step - loss: 3.4753 - val_loss: 2.8282\n",
            "Epoch 2/5\n",
            "72/72 [==============================] - 3s 31ms/step - loss: 2.7048 - val_loss: 2.5678\n",
            "Epoch 3/5\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 2.5607 - val_loss: 2.4815\n",
            "Epoch 4/5\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.4768 - val_loss: 2.4026\n",
            "Epoch 5/5\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.4039 - val_loss: 2.3440\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 2.3432\n",
            "eval_loss: 2.343165397644043\n",
            "perplexity 10.414149368066521\n",
            "\n",
            "Вой го мо пой пой ми ой,\n",
            "Во пой насё пой,\n",
            "Ви пой мей раза вы вой пой пой,\n",
            "Ты,\n",
            "Пой,\n",
            "Пой гой ой бой сто помой бы ской стожестыхой пой ста помой пой – за ной вой бой пой ско ны ть ми вомой вой ны пой стара вы по стой ви пой дой дой пой вена, похой пой за стой да пой пой.\n",
            "На зай ой ста сё пой вой на пелой вой,\n",
            "И во пой вой дой пой пой вой пой пой пой зато вый мита вы, помемой зой вы ви пой и пра бой, мужи мой та да пой поми пой чу рыхой бе вой сты сть помий да той пой и пой веть сте,\n",
            "Воми вы мой сте на верамей стой ми ны пожена сё той могой стамиходей гой пой ть ве за ой стра,\n",
            "Свой по мой мой вогостой,\n",
            "Наза пой пой пой дай у дой по пой пой сё во побы ской подамой ной,\n",
            "Пой пой меся на пой по ихажи ой и ой пой бе мой та вогожи по пой пой пой мой зася пой вы потой застой стазасё пой вей сколой пой бы сё у пой пой бомой бый у погой ой,\n",
            "Бе ихой,\n",
            "Аль Ска,\n",
            "И во сё ви той сть той пой бы пой вы бы огой па омой стой сё ой пой сё пой стей вы стой дой вовы стой сти сё пой стостой на ся пой сте уми ла \n",
            "________________________________________________________________________________\n",
            "\n",
            "run time: 2.9914872646331787\n",
            "Epoch 1/15\n",
            "72/72 [==============================] - 10s 76ms/step - loss: 3.4635 - val_loss: 2.7962\n",
            "Epoch 2/15\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 2.6743 - val_loss: 2.5490\n",
            "Epoch 3/15\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.5383 - val_loss: 2.4634\n",
            "Epoch 4/15\n",
            "72/72 [==============================] - 3s 25ms/step - loss: 2.4526 - val_loss: 2.3848\n",
            "Epoch 5/15\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.3845 - val_loss: 2.3246\n",
            "Epoch 6/15\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.3292 - val_loss: 2.2870\n",
            "Epoch 7/15\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.2806 - val_loss: 2.2342\n",
            "Epoch 8/15\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.2343 - val_loss: 2.1950\n",
            "Epoch 9/15\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.1903 - val_loss: 2.1517\n",
            "Epoch 10/15\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 2.1474 - val_loss: 2.1126\n",
            "Epoch 11/15\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.1104 - val_loss: 2.0750\n",
            "Epoch 12/15\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0746 - val_loss: 2.0447\n",
            "Epoch 13/15\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0427 - val_loss: 2.0208\n",
            "Epoch 14/15\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0138 - val_loss: 1.9993\n",
            "Epoch 15/15\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9882 - val_loss: 1.9733\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.9601\n",
            "eval_loss: 1.9601351022720337\n",
            "perplexity 7.100286265166637\n",
            "\n",
            "Во на сты ной, сты веве но вой сты веть сти сти продето нуты на пост стый побо сто прено стосты стове по сти поде пой сте сты подета таленоде ста сто ми на сты нена прой ста скастодена ста стой по пой на востой сто ле но пой стой лестви сто стасты мой ста пой на стой та сты на ве ста нодесты ста сто пове стой той пой сто сты ты ностой,\n",
            "Ска стодеты стра ся стой ветой дереты сты сты стогой вено бой пой сты ско ме подестамой сты сто на ся ностой ни ты вести постомой стой на сто сть по на сто пой ска во ветескощи леного на стой ны пой стросто ств м пости скастра сты постах ве по пой ста ме сто пост но сты ста ве сты стодето на пеле стой ствета ста пра за сты ска стой ной подра сты скутой ста бой мы на ты ве стены стой вой,\n",
            "Скоста муто ме стовой м вой намутаско но ствей стой сты но по но сто тенале ле нали вели стам стой ста ста ной лена же в сты на пой ствеста нать по на ла ной медо ве строй стоской ского по сто мо сто ста на поска пой по стой носкому по по по навой вой ветосто ни на стото\n",
            "________________________________________________________________________________\n",
            "\n",
            "run time: 3.54909348487854\n",
            "Epoch 1/30\n",
            "72/72 [==============================] - 7s 65ms/step - loss: 3.4429 - val_loss: 2.7744\n",
            "Epoch 2/30\n",
            "72/72 [==============================] - 4s 44ms/step - loss: 2.6674 - val_loss: 2.5499\n",
            "Epoch 3/30\n",
            "72/72 [==============================] - 3s 22ms/step - loss: 2.5425 - val_loss: 2.4643\n",
            "Epoch 4/30\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 2.4647 - val_loss: 2.4007\n",
            "Epoch 5/30\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.3945 - val_loss: 2.3313\n",
            "Epoch 6/30\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.3308 - val_loss: 2.2719\n",
            "Epoch 7/30\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 2.2786 - val_loss: 2.2289\n",
            "Epoch 8/30\n",
            "72/72 [==============================] - 3s 22ms/step - loss: 2.2298 - val_loss: 2.1882\n",
            "Epoch 9/30\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.1862 - val_loss: 2.1494\n",
            "Epoch 10/30\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.1455 - val_loss: 2.1106\n",
            "Epoch 11/30\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.1081 - val_loss: 2.0774\n",
            "Epoch 12/30\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.0740 - val_loss: 2.0491\n",
            "Epoch 13/30\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 2.0424 - val_loss: 2.0200\n",
            "Epoch 14/30\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.0145 - val_loss: 2.0011\n",
            "Epoch 15/30\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9897 - val_loss: 1.9749\n",
            "Epoch 16/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9657 - val_loss: 1.9598\n",
            "Epoch 17/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9440 - val_loss: 1.9419\n",
            "Epoch 18/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9253 - val_loss: 1.9260\n",
            "Epoch 19/30\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.9067 - val_loss: 1.9150\n",
            "Epoch 20/30\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.8904 - val_loss: 1.8999\n",
            "Epoch 21/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8763 - val_loss: 1.8912\n",
            "Epoch 22/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8620 - val_loss: 1.8831\n",
            "Epoch 23/30\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.8481 - val_loss: 1.8777\n",
            "Epoch 24/30\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.8367 - val_loss: 1.8681\n",
            "Epoch 25/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8246 - val_loss: 1.8622\n",
            "Epoch 26/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8150 - val_loss: 1.8527\n",
            "Epoch 27/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8038 - val_loss: 1.8459\n",
            "Epoch 28/30\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7931 - val_loss: 1.8426\n",
            "Epoch 29/30\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.7844 - val_loss: 1.8376\n",
            "Epoch 30/30\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.7758 - val_loss: 1.8331\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 1.8153\n",
            "eval_loss: 1.8153338432312012\n",
            "perplexity 6.143126676412626\n",
            "\n",
            "Ка у стой,\n",
            "Вой веледни сто строй стой строй у пот пой страме потой вето сто ст ны пото по стодеброй стой сто но потого сти стотода сто стотой не в ното следе сто ст на сче мемо стой страка но пой кой стой ста сто стой мой по пот стро сть но се ст то сти стой ста мушилетой побе стый ве сто стамо ла ве строно пой сто сла се ве про сто на ся сти се сты стра мой ный ст пра ст пой в стый вотый стото но на ко вой ты бе стой,\n",
            "Норотой ной,\n",
            "Тый стода стой стой веротой, ст ный ни нетот стовенам милой мо ста на стоно стый, ст нототи пой ст сча нотром на сь настой ска сто стой пото стостой то пой по стоней ст сто м нотой стой строна пой стосто прого днеро ный м стотой строто стоторатой нодото сто сти стоде по гой стото сет пой ной, сть ве ве нот,\n",
            "Ку но сь стой вестротой сто ст сто по ся ст новоретугоро стой вене м ста стый.\n",
            "И стотот дотой стой се стого стой м ве стра поме скототи потой ст по сто сла сть сто не нот вето стото стила стотой сто в ст скорака стоты порой не вый стобе стытой нотосе стот\n",
            "________________________________________________________________________________\n",
            "\n",
            "run time: 2.807809591293335\n",
            "Epoch 1/50\n",
            "72/72 [==============================] - 9s 87ms/step - loss: 3.4900 - val_loss: 2.8281\n",
            "Epoch 2/50\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 2.7052 - val_loss: 2.5658\n",
            "Epoch 3/50\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.5588 - val_loss: 2.4740\n",
            "Epoch 4/50\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.4714 - val_loss: 2.4039\n",
            "Epoch 5/50\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 2.4037 - val_loss: 2.3513\n",
            "Epoch 6/50\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 2.3485 - val_loss: 2.2973\n",
            "Epoch 7/50\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.2986 - val_loss: 2.2513\n",
            "Epoch 8/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.2514 - val_loss: 2.2092\n",
            "Epoch 9/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.2086 - val_loss: 2.1687\n",
            "Epoch 10/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.1666 - val_loss: 2.1303\n",
            "Epoch 11/50\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 2.1286 - val_loss: 2.0959\n",
            "Epoch 12/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0933 - val_loss: 2.0632\n",
            "Epoch 13/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.0596 - val_loss: 2.0365\n",
            "Epoch 14/50\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.0296 - val_loss: 2.0113\n",
            "Epoch 15/50\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.0028 - val_loss: 1.9872\n",
            "Epoch 16/50\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.9771 - val_loss: 1.9677\n",
            "Epoch 17/50\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9541 - val_loss: 1.9443\n",
            "Epoch 18/50\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9340 - val_loss: 1.9298\n",
            "Epoch 19/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9142 - val_loss: 1.9165\n",
            "Epoch 20/50\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.8970 - val_loss: 1.9023\n",
            "Epoch 21/50\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.8807 - val_loss: 1.8946\n",
            "Epoch 22/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8644 - val_loss: 1.8890\n",
            "Epoch 23/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8513 - val_loss: 1.8768\n",
            "Epoch 24/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8394 - val_loss: 1.8648\n",
            "Epoch 25/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8260 - val_loss: 1.8629\n",
            "Epoch 26/50\n",
            "72/72 [==============================] - 3s 21ms/step - loss: 1.8143 - val_loss: 1.8525\n",
            "Epoch 27/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8041 - val_loss: 1.8511\n",
            "Epoch 28/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7940 - val_loss: 1.8411\n",
            "Epoch 29/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7838 - val_loss: 1.8318\n",
            "Epoch 30/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7745 - val_loss: 1.8331\n",
            "Epoch 31/50\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.7653 - val_loss: 1.8287\n",
            "Epoch 32/50\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.7576 - val_loss: 1.8240\n",
            "Epoch 33/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7484 - val_loss: 1.8267\n",
            "Epoch 34/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7415 - val_loss: 1.8184\n",
            "Epoch 35/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7334 - val_loss: 1.8138\n",
            "Epoch 36/50\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.7262 - val_loss: 1.8161\n",
            "Epoch 37/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7202 - val_loss: 1.8133\n",
            "Epoch 38/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7127 - val_loss: 1.8047\n",
            "Epoch 39/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7075 - val_loss: 1.8034\n",
            "Epoch 40/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6999 - val_loss: 1.8043\n",
            "Epoch 41/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6951 - val_loss: 1.8034\n",
            "Epoch 42/50\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.6892 - val_loss: 1.8021\n",
            "Epoch 43/50\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.6840 - val_loss: 1.8014\n",
            "Epoch 44/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6776 - val_loss: 1.7992\n",
            "Epoch 45/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6718 - val_loss: 1.7967\n",
            "Epoch 46/50\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6690 - val_loss: 1.7958\n",
            "Epoch 47/50\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6637 - val_loss: 1.8018\n",
            "Epoch 48/50\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6576 - val_loss: 1.7991\n",
            "Epoch 49/50\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.6525 - val_loss: 1.7993\n",
            "Epoch 50/50\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6481 - val_loss: 1.7947\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.7742\n",
            "eval_loss: 1.7741563320159912\n",
            "perplexity 5.895305357275527\n",
            "\n",
            "Нодена стоде ста стой т велой вела стой пой стовесто вый сти мо мой ве м не по,\n",
            "Стой стови сто но ви м ве пой ка ста пра на стой в веда стода в на ве сто веро витой сто сте на стой м ви про сто вой да сто ской стой но стой да стой в стой сто устоветь вени пра ве стой пора не мо вова нера стотой вестой вой сте той на в сто не вой ветостой стой ве вила ви ми вена вимора и в нана ва на водугой стой сть сто ве вой ве про во стой в сто ви стой той уне ной,\n",
            "Ве по дый стой та стом по ни м стосто прододни то по ки слеродода стода ви ве ст стой на сто сто ви ме сле сто стой ни ва на ве слена м сть сто но сти сто по м меда сто та стом муго ной на прато вой у сто но стой м стой дра стой ве и ве стра веду натодуго ст стой стой стой м де за в ве мой и стостой пра т ст пой вени нетостото в стой ви спорой стой ва стой ста стостой сто стой стода ный вый стоденый в ве пом в пе стоде пра но вой сто пой вый стобони в в зано сть ной ветодрото за ст там вода сти сто на сто нода сто ито на пой мой нь в ве в\n",
            "________________________________________________________________________________\n",
            "\n",
            "run time: 2.8196969032287598\n",
            "Epoch 1/70\n",
            "72/72 [==============================] - 9s 83ms/step - loss: 3.4773 - val_loss: 2.8257\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 3s 32ms/step - loss: 2.6920 - val_loss: 2.5552\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 2.5456 - val_loss: 2.4654\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.4604 - val_loss: 2.3937\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.3958 - val_loss: 2.3362\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.3410 - val_loss: 2.2891\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.2910 - val_loss: 2.2459\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 2.2453 - val_loss: 2.2067\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 2.2013 - val_loss: 2.1578\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.1601 - val_loss: 2.1192\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.1227 - val_loss: 2.0901\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 2.0879 - val_loss: 2.0572\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.0570 - val_loss: 2.0301\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.0276 - val_loss: 2.0058\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0015 - val_loss: 1.9847\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9769 - val_loss: 1.9662\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.9544 - val_loss: 1.9493\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.9345 - val_loss: 1.9322\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.9163 - val_loss: 1.9219\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8986 - val_loss: 1.9061\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8828 - val_loss: 1.8978\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8678 - val_loss: 1.8852\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.8539 - val_loss: 1.8766\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8413 - val_loss: 1.8689\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8285 - val_loss: 1.8609\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8179 - val_loss: 1.8585\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8059 - val_loss: 1.8494\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7968 - val_loss: 1.8400\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.7865 - val_loss: 1.8408\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.7763 - val_loss: 1.8362\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7676 - val_loss: 1.8356\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7607 - val_loss: 1.8308\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7529 - val_loss: 1.8224\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7451 - val_loss: 1.8198\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.7367 - val_loss: 1.8176\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.7299 - val_loss: 1.8147\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7234 - val_loss: 1.8134\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7169 - val_loss: 1.8091\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7097 - val_loss: 1.8122\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.7044 - val_loss: 1.8087\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.6980 - val_loss: 1.8068\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6919 - val_loss: 1.8051\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6876 - val_loss: 1.8040\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6799 - val_loss: 1.7991\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6757 - val_loss: 1.8014\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.6710 - val_loss: 1.7977\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6667 - val_loss: 1.7975\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6630 - val_loss: 1.7970\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6566 - val_loss: 1.7987\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6522 - val_loss: 1.7955\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.6464 - val_loss: 1.7966\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.6439 - val_loss: 1.7935\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6388 - val_loss: 1.7983\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6354 - val_loss: 1.7935\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6326 - val_loss: 1.7974\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.6279 - val_loss: 1.7962\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6232 - val_loss: 1.8036\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6209 - val_loss: 1.7938\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6161 - val_loss: 1.7923\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6123 - val_loss: 1.7966\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6087 - val_loss: 1.7961\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.6057 - val_loss: 1.7961\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6015 - val_loss: 1.7973\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.5990 - val_loss: 1.7969\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5973 - val_loss: 1.7967\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.5926 - val_loss: 1.7958\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 3s 22ms/step - loss: 1.5892 - val_loss: 1.8024\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.5863 - val_loss: 1.7989\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.5829 - val_loss: 1.8016\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5812 - val_loss: 1.7998\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.7707\n",
            "eval_loss: 1.7706831693649292\n",
            "perplexity 5.874865518880588\n",
            "\n",
            "Ной от ве пой стодено ми стоде стоми м, стодани ви стом мой стот стотоде ве нодни ст стой ст ста пра ст вом нала стото м стою стой стве стом ст ст ст ста ся ст бо стой стом ни сто сле прой ни стой ст мом но стом сто стой ста ст стой мо сто ст стодом ск ст ве ве скодене ве сте ве ск скист ть стой но ть ни той ве ск по стодой стой,\n",
            "Вени ст ста стой сла сви ст стоде ст мугом ск,\n",
            "Ве мой сто стой сте сто ст но вост т стой номеном стетой стой сти ни стой по пой стоде сторода в слито стомоде ви ност м ст, ста в стый,\n",
            "Ска ст ст ста не стоду вени ст ста сто сто сте ся,\n",
            "И ви ве ста сту прой ст сто ск ст м ста ста в ст ст ном во ном стово ст сте стотоднене ст не ст пой поде стра ст м ста же м стистой,\n",
            "Вет мом сть то сто сте в де ве сто ви вый стой м де ст помой ве ст в стой стой по во м ста новой сти воде ве м сть стой т.\n",
            "Ве ст дной пой во сто по посто ст ст ской ве прела нала м м пе ст м сторе ве ст стороденый стой,\n",
            "Во и ст сто стой ст ноде по ста вей ст пора пой ст ст ви сте ся и ст ветосто сто\n",
            "________________________________________________________________________________\n",
            "\n",
            "run time: 2.8422563076019287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Jr3d4Vlww5Ar",
        "outputId": "ebb2406c-46cb-4f00-9a04-494f2d5f40b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          eval_loss perplexity  \\\n",
              "Epochs 5   2.343165  10.414149   \n",
              "Epochs 15  1.960135   7.100286   \n",
              "Epochs 30  1.815334   6.143127   \n",
              "Epochs 50  1.774156   5.895305   \n",
              "Epochs 70  1.770683   5.874866   \n",
              "\n",
              "                                                 result_text  run_time  \n",
              "Epochs 5   \\nВой го мо пой пой ми ой,\\nВо пой насё пой,\\n...  2.991487  \n",
              "Epochs 15  \\nВо на сты ной, сты веве но вой сты веть сти ...  3.549093  \n",
              "Epochs 30  \\nКа у стой,\\nВой веледни сто строй стой строй...   2.80781  \n",
              "Epochs 50  \\nНодена стоде ста стой т велой вела стой пой ...  2.819697  \n",
              "Epochs 70  \\nНой от ве пой стодено ми стоде стоми м, стод...  2.842256  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dca8e31e-efdf-4f53-ab07-a34ff6976fc7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eval_loss</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>result_text</th>\n",
              "      <th>run_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Epochs 5</th>\n",
              "      <td>2.343165</td>\n",
              "      <td>10.414149</td>\n",
              "      <td>\\nВой го мо пой пой ми ой,\\nВо пой насё пой,\\n...</td>\n",
              "      <td>2.991487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Epochs 15</th>\n",
              "      <td>1.960135</td>\n",
              "      <td>7.100286</td>\n",
              "      <td>\\nВо на сты ной, сты веве но вой сты веть сти ...</td>\n",
              "      <td>3.549093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Epochs 30</th>\n",
              "      <td>1.815334</td>\n",
              "      <td>6.143127</td>\n",
              "      <td>\\nКа у стой,\\nВой веледни сто строй стой строй...</td>\n",
              "      <td>2.80781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Epochs 50</th>\n",
              "      <td>1.774156</td>\n",
              "      <td>5.895305</td>\n",
              "      <td>\\nНодена стоде ста стой т велой вела стой пой ...</td>\n",
              "      <td>2.819697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Epochs 70</th>\n",
              "      <td>1.770683</td>\n",
              "      <td>5.874866</td>\n",
              "      <td>\\nНой от ве пой стодено ми стоде стоми м, стод...</td>\n",
              "      <td>2.842256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dca8e31e-efdf-4f53-ab07-a34ff6976fc7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dca8e31e-efdf-4f53-ab07-a34ff6976fc7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dca8e31e-efdf-4f53-ab07-a34ff6976fc7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in dict_epochs_describe.items():\n",
        "  print(key, value, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl7NE38IxKRb",
        "outputId": "6b10fbb3-c92d-4cb9-8626-0e64cd51d904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs 5\n",
            "         len  lines  mean_line_len\n",
            "count    1.0    1.0       1.000000\n",
            "mean   999.0   14.0      65.666667\n",
            "std      NaN    NaN            NaN\n",
            "min    999.0   14.0      65.666667\n",
            "25%    999.0   14.0      65.666667\n",
            "50%    999.0   14.0      65.666667\n",
            "75%    999.0   14.0      65.666667\n",
            "max    999.0   14.0      65.666667\n",
            "Epochs 15\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0       1.000000\n",
            "mean   1000.0    2.0     332.666667\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0    2.0     332.666667\n",
            "25%    1000.0    2.0     332.666667\n",
            "50%    1000.0    2.0     332.666667\n",
            "75%    1000.0    2.0     332.666667\n",
            "max    1000.0    2.0     332.666667\n",
            "Epochs 30\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0       1.000000\n",
            "mean   1000.0    5.0     165.833333\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0    5.0     165.833333\n",
            "25%    1000.0    5.0     165.833333\n",
            "50%    1000.0    5.0     165.833333\n",
            "75%    1000.0    5.0     165.833333\n",
            "max    1000.0    5.0     165.833333\n",
            "Epochs 50\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0       1.000000\n",
            "mean   1000.0    2.0     332.666667\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0    2.0     332.666667\n",
            "25%    1000.0    2.0     332.666667\n",
            "50%    1000.0    2.0     332.666667\n",
            "75%    1000.0    2.0     332.666667\n",
            "max    1000.0    2.0     332.666667\n",
            "Epochs 70\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0          1.000\n",
            "mean   1000.0    7.0        124.125\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0    7.0        124.125\n",
            "25%    1000.0    7.0        124.125\n",
            "50%    1000.0    7.0        124.125\n",
            "75%    1000.0    7.0        124.125\n",
            "max    1000.0    7.0        124.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По статистическим характеристикам сложно сделать какой-либо вывод, так как при любом количестве эпох модель находит только одно стихотворение. С увеличением количества эпох перплеския и ошибка на тестовых данных уменьшается, что говорит о том, что оптимальное количество эпох - 70."
      ],
      "metadata": {
        "id": "vm9WNvuGq8U3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Изменяя параметр температуры T проанализируем изменения сгенерированного текста. Выберем оптимальное значение параметра."
      ],
      "metadata": {
        "id": "3eHAFnvBxulZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = [0.1, 0.3, 0.5, 0.7, 1.0]"
      ],
      "metadata": {
        "id": "njbJJowPx3PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in T:\n",
        "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "    start = time.time()\n",
        "    states = None\n",
        "    next_char = tf.constant(['\\n'])\n",
        "    result = [next_char]\n",
        "\n",
        "    for n in range(N):\n",
        "        next_char, states = one_step_model.generate_one_step(next_char, states=states, temperature=i)\n",
        "        result.append(next_char)\n",
        "\n",
        "    result = tf.strings.join(result)\n",
        "    end = time.time()\n",
        "\n",
        "    result_text = result[0].numpy().decode('utf-8')\n",
        "    print(f\"T = {i}\", result_text, sep='\\n')\n",
        "    print('_'*80)\n",
        "    print('\\nRun time:', end - start)\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujb4I9UOyPg1",
        "outputId": "c970c85d-d322-45f1-8761-0069dc9874da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T = 0.1\n",
            "\n",
            "Ве стой стой но стой стой стой ст стой сте стой стой стой стой стой стой стой ст сто стой сто ст ст стой сто ст стой сто сто сто сто ст стой ст стой ст стой стой ст ст ст ст стой сто стой стой стой стой сто стой стой стой ст стой стой стой стой стой стой сто ст сто стой сто сто сто сто стой стой сто стой пой стой ст сто стой ст стой ст ст стой стой стой стой стой стой ст стой стой стой стой ст ст сто стой сто ст стой ст сто стой ст стом сто ст ст стой ст сто ве стой ста стоде сто стой стой стой стой сто стой стой стой стой стой стой по стой сто сто стой сто стой стой стой сто сто стой сто стой стой стой стой стой сто стой сто сто сто стой стой стой ве стой стой сто сто стой стой стой стой стой стой стой сто сто стой сто стой стой сто стоде стой стой сто стой сто стой стой стой стой стой стой сто стой стой стой ст ве стой ст стой стой стой пой сто сто стой сто сто стостом стой стой сто ст сто стой стой стоде стой сто ст стой стой стой стой стой сто стой стой стой ст стой стой ве стомо с\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.9121508598327637\n",
            "\n",
            "\n",
            "T = 0.3\n",
            "\n",
            "Ви сле и стоме вой, стеный вет стой сте стом одет стоме ст ся ски нене ве стаза сто вой пой велой и ста сто пра сто витомостугой сто ст мисть ст ст стере стой пой сто ст ста поде и ст ной ст сто стом стом тора ск,\n",
            "И в мом м сто пра т ст м ви ст пой стой м ри сла ста сто вене ностода бра по ва ви стой т ст во ст ста стой ст ся в стей в ста стой пой слалито меда не ст стотоной ся ве по во ста ст сть ст ст сто ст гой ст вом могой мотой му сто сте стой ном ской сла м пой но стола ст м го ст вом стой ви ви сте ви и пом стодесте ни поме сто сле стестостода сто стой нит стой ви м скоми сто в стой ве венеде сте ста ской ст сте м ве пой стой стоде вый сю веноко сти вотодный м ветой,\n",
            "Нот стою ст мо сто сте стом сто ве скра ной сто сть ся,\n",
            "Сто стве скорой не м ств ст ст сте вето пой стой му стоде по сто мут ской ством ст не ст в стой ск ой ст во стой ни вой сто де вой пой стовой по постоде скоскода стой стом стомо ной во но ст м ве стом ной вела ве м ст стом по пра стостодеза стото ве ми пой стод\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.847640037536621\n",
            "\n",
            "\n",
            "T = 0.5\n",
            "\n",
            "Тою вый стой равили вочей пралодамера,\n",
            "Слат постый ск ст оми жере повитост ворих</s седера вим ве вепрою помом ся ме ст\n",
            "Ненилежи ся смой за ст сть т ми сть,\n",
            "Кинею вой сный помить не вой вой вей ся спо витой\n",
            "Стака ны сто слой, зди бо вом м ствеча бугорой н не ветопоской ру, препо пробя слой погою ве сеной слою стою вст ны мый\n",
            "Ода пом ому спрой ветотомова по воюбирой скомаманы пра в сле сь в и претеда сть пой, сла та ст мою сти ную ска вит ной сть мужит ск сни бя вочасм и м незако стря ста сла м;\n",
            "\n",
            "Ком стетой стою поктоканот ст вы пра бот;\n",
            "Ненодродовый сте ми ся, бый сла сера нисты ной,\n",
            "Неназавеной седена угоде ся спроду сти про стете м с недала ликора м стот ла стой,\n",
            "В ти сеца\n",
            "Веска сль столите ненеде стезам сли,\n",
            "Занота во м веля.\n",
            "Тый ни осте за м нь стетодому\n",
            "Ей стота м сеный вегой мута той стой стом ст мет бя\n",
            "Ве ста сти моный, мовосерукодемаша ста м всло вь в по и вень го пой зане да одой дени, вома вав нененоза помо м ст ст стой,\n",
            "Чумо д вей стомо нодетот пот,\n",
            "Веной я сторо слостый пос\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.883580446243286\n",
            "\n",
            "\n",
            "T = 0.7\n",
            "\n",
            "Одам\n",
            "Несводугрузнока чуша счамумужечамеминой м см натый м мо налый сть пой ще понесчистый,\n",
            "Ваной сибелелеле стиследутак тост м вена т чепрода,\n",
            "Ужене поздериро моче сло дня висчанытошуглолю прото глоза н о итаде я совой ся ве нытнелет посчакогой вя мераморать несеты,\n",
            "Отою вом бя о, здесяте сле кики – сплоезв потослородет сти бес порей боть нех жит мый,\n",
            "Угодивой, сят стодистить стей, ми стой,\n",
            "Амы…\n",
            "Визби нила уча е,\n",
            "И понотой ся нь вогу на лиренослироснетося комодера вы за сля тих мость поглицаласк,\n",
            "Недахо ст в угост нет овсми\n",
            "И у бве прегобой.\n",
            "Илакосмем, спорам здневе бой ив ча,\n",
            "Зара ноть омоне ный ся нися, мов груся стугли нью така нале нось седи поней тером полазде т.\n",
            "Небосторахитовей у —\n",
            "Пой превети м да о ди сли счт вошалой,\n",
            "И бривы,\n",
            "Вей днистала ираск,\n",
            "Вей ный,\n",
            "\n",
            "Не иротося монь пом ся пивет ный прилослодогора счу нубоны\n",
            "Ба камодот тва ве вери сора сла во дономны, вый стой ся сися угось та прому сезвем е по ному итивето но неменаванетодеда!\n",
            "Вдоме пой дези ст.\n",
            "Чузлоны, бота!.\n",
            "И се дер\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.273123741149902\n",
            "\n",
            "\n",
            "T = 1.0\n",
            "\n",
            "Сужею негоюмистыпостаст етром нешаруй в увый\n",
            "Поря\n",
            "Ветвов ирнерый ои сё\n",
            "Усповыли оребивя,\n",
            "<».\n",
            "Изи,\n",
            "Каров нила, омослика в: музнь твем?>\n",
            "Хом тиви нороре ск никугидовазвир</s\n",
            "Бе,\n",
            "\n",
            "Ум</s —\n",
            "И ме се егой сланет ме х у к но в – спити,\n",
            "Оро окоспрутаза лая ми свиспест;\n",
            "Ныйемостих ре рему\n",
            "Звот\n",
            "Ти ла ся ра,\n",
            "\n",
            "Сплом скорумый пой ся.\n",
            "Нездедачу сянегоме, м\n",
            "Ста вамовлю-тыт;\n",
            "Ск:\n",
            "\n",
            "В робсаени навре, мних:\n",
            "\n",
            "Неснны.\n",
            "За завели,\n",
            "\n",
            "_\n",
            "Сти пед и\n",
            "Сларетыналерлим?.\n",
            "Изля рот\n",
            "Синь.\n",
            "Такашодн бог И ве\n",
            "Зошилы,\n",
            "Сли тв снь ной ц, и бе<?rшалисных бойведра костезалой\n",
            "Лаже– вныю ведрочь,\n",
            "Жит н одне.\n",
            "Бех ставь новоча теруглому позо,\n",
            "Яю вх зора поралочи слумы вен, с вепуститебодуж жес у, яшодривылуж позлодь них.\n",
            "И эme ваде.\n",
            "Они\n",
            "В твя бос у бл«<?»</s\n",
            "У би пежиме торзрить рочебя м на в, ра м ни! вди:\n",
            "Ужедивебра у пой плаеста м.\n",
            "Дей дцешаля,\n",
            "Перь?\n",
            "Пою,\n",
            "Ужетешь – прзирака ныеби;\n",
            "Мет кна пола, Мены бнею севзарале,\n",
            "Нетсе м м\" апсьиéася\n",
            "ИU) а деднало Фск н.\n",
            "Норегомит —\n",
            "Беескомакумщегнисв деча ки!\n",
            "«В кежер,\n",
            "Плилездст ашкилах,\n",
            "Виск-сп\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.810793399810791\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визуально при T=0.7 текст имеет более разумный вид"
      ],
      "metadata": {
        "id": "aH4DNPE0ztNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Проанализируем зависимость перплексии, скорости обучения, результатов генерации от параметров нейронной сети embedding_dim, gru_units: embedding_dim = {vocab/4, vocab/2, vocab, vocab * 2, vocab * 4}, где vocab = размер словаря выборки. gru_units = {10, 100, 300, 500}"
      ],
      "metadata": {
        "id": "ziLsZOCKz6A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = [vocab_size/4, vocab_size/2, vocab_size, vocab_size * 2, vocab_size * 4]\n",
        "embedding_dim = list(map(int, embedding_dim))\n",
        "gru_units = 300\n",
        "T = 0.7\n",
        "NUM_EPOCHS = 70"
      ],
      "metadata": {
        "id": "gQ3cK15q0joI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_embedding_dim = pd.DataFrame(\n",
        "    columns=['eval_loss', 'perplexity', 'result_text', 'run_time'], \n",
        "    index=['embedding_dim ' + str(i) for i in embedding_dim])\n",
        "dict_embedding_dim_describe = {}"
      ],
      "metadata": {
        "id": "SHcI_Xgq0HiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in embedding_dim:\n",
        "    model = MyModel(\n",
        "        vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "        embedding_dim=i,\n",
        "        gru_units=gru_units)\n",
        "    model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset, \n",
        "        validation_data = val_dataset, \n",
        "        epochs=NUM_EPOCHS, \n",
        "        callbacks=[checkpoint_callback])\n",
        "    \n",
        "    eval_loss = model.evaluate(test_dataset)\n",
        "    perplexity = np.exp(eval_loss)\n",
        "    print('eval_loss:', eval_loss)\n",
        "    print('perplexity', perplexity)\n",
        "\n",
        "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "    start = time.time()\n",
        "    states = None\n",
        "    next_char = tf.constant(['\\n'])\n",
        "    result = [next_char]\n",
        "\n",
        "    for n in range(N):\n",
        "        next_char, states = one_step_model.generate_one_step(next_char, states=states, temperature=T)\n",
        "        result.append(next_char)\n",
        "\n",
        "    result = tf.strings.join(result)\n",
        "    end = time.time()\n",
        "\n",
        "    result_text = result[0].numpy().decode('utf-8')\n",
        "    print(result_text)\n",
        "    print('_'*80)\n",
        "    run_time = end - start\n",
        "    print('\\nRun time:', run_time)\n",
        "\n",
        "    df_embedding_dim.loc[f'embedding_dim {i}'] = [eval_loss, perplexity, result_text, run_time]\n",
        "    dict_embedding_dim_describe[f'embedding_dim {i}'] = describe_poems(result_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTxdPKBC2o4P",
        "outputId": "caf144e3-1087-433f-d4c3-56b21412852d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "72/72 [==============================] - 10s 82ms/step - loss: 3.7357 - val_loss: 3.3922\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 4s 37ms/step - loss: 3.1446 - val_loss: 2.8380\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 5s 36ms/step - loss: 2.8102 - val_loss: 2.6610\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 5s 36ms/step - loss: 2.6909 - val_loss: 2.5857\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 5s 37ms/step - loss: 2.6257 - val_loss: 2.5366\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 2.5776 - val_loss: 2.4961\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 2.5373 - val_loss: 2.4603\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 2.5027 - val_loss: 2.4225\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.4732 - val_loss: 2.3922\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 3s 23ms/step - loss: 2.4454 - val_loss: 2.3657\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.4187 - val_loss: 2.3401\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 2.3943 - val_loss: 2.3165\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 3s 25ms/step - loss: 2.3681 - val_loss: 2.2903\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 2.3455 - val_loss: 2.2670\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.3198 - val_loss: 2.2385\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.3001 - val_loss: 2.2213\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.2770 - val_loss: 2.2079\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.2578 - val_loss: 2.1766\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.2357 - val_loss: 2.1591\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 2.2170 - val_loss: 2.1374\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.1985 - val_loss: 2.1226\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.1803 - val_loss: 2.1057\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.1648 - val_loss: 2.0863\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.1479 - val_loss: 2.0785\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.1348 - val_loss: 2.0619\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.1193 - val_loss: 2.0510\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 2.1065 - val_loss: 2.0384\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0937 - val_loss: 2.0286\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.0804 - val_loss: 2.0167\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0693 - val_loss: 2.0091\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.0582 - val_loss: 1.9992\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0480 - val_loss: 1.9858\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0369 - val_loss: 1.9766\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.0284 - val_loss: 1.9740\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0167 - val_loss: 1.9665\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0098 - val_loss: 1.9594\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0006 - val_loss: 1.9540\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.9944 - val_loss: 1.9484\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9864 - val_loss: 1.9408\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.9783 - val_loss: 1.9352\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9696 - val_loss: 1.9300\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.9632 - val_loss: 1.9263\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9567 - val_loss: 1.9224\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.9513 - val_loss: 1.9191\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9453 - val_loss: 1.9140\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9379 - val_loss: 1.9076\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.9322 - val_loss: 1.9099\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9274 - val_loss: 1.9038\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.9230 - val_loss: 1.9015\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.9176 - val_loss: 1.8957\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.9135 - val_loss: 1.8938\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 1.9067 - val_loss: 1.8945\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9013 - val_loss: 1.8864\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.8981 - val_loss: 1.8838\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 3s 15ms/step - loss: 1.8941 - val_loss: 1.8861\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8885 - val_loss: 1.8823\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8837 - val_loss: 1.8782\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 4s 27ms/step - loss: 1.8797 - val_loss: 1.8813\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.8742 - val_loss: 1.8763\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.8703 - val_loss: 1.8752\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 1.8692 - val_loss: 1.8711\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8642 - val_loss: 1.8721\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8616 - val_loss: 1.8697\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8582 - val_loss: 1.8741\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.8546 - val_loss: 1.8638\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8511 - val_loss: 1.8635\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8467 - val_loss: 1.8606\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8432 - val_loss: 1.8636\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8426 - val_loss: 1.8586\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.8386 - val_loss: 1.8586\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.8426\n",
            "eval_loss: 1.8426249027252197\n",
            "perplexity 6.313087772393886\n",
            "\n",
            "Огезродвопо нь стакой ва коме па бе звсть ни угатокавы стни,\n",
            "Ненобь ной ой поe, вебена;\n",
            "Натя стою, во вочепратаючецом и бу,\n",
            "Ности м и ваю, на ненопедедобуск Кате вонета робевоки, сле нилила.\n",
            "Вяй,\n",
            "Слы,\n",
            "Пренедивой\n",
            "Сл?»И гобашаро нуты\n",
            "И ге кагогоро,\n",
            "И машащи бой вовлГрелВы,\n",
            "Вы преложера м –\n",
            "Огоскопатак, роду ниводожа пой тист,\n",
            "И бев вази слено рога вень недубужемане на м трола та;\n",
            "Небя помезуча не дотедатобетасе гимебу по тогобя,\n",
            "Ум мек сть нетитоней стопе сте хои ма илищиломит ской,\n",
            "Ва лираруo воты тиломитостых ча, сть ма меколи ватадалодот,\n",
            "Плый н тогронебуспрой,\n",
            "Ой и миты,\n",
            "\n",
            "Немилезде кобь тый;\n",
            "Прогди спобостазожеда во скода зажи не смени доротона пратаста в венок де сте педем смот н,\n",
            "</s> тый пенея тоныпри сьрой p\n",
            "\n",
            "Ти полинане на назнеролехнетам стю, «Вой, пой ста пой ва\n",
            "Ожотоди ме та олавоткиль,\n",
            "Да ной ваной при;\n",
            "Побь та нень\n",
            "Тни,\n",
            "Тужа войнезгота ве сё ста восях сль,\n",
            "Ноюточата водек ностый,\n",
            "Пеля ня и натишинени ть ва сты ся продезра по вих узра воруточа пой, роден пусти пуколы сста др\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.8830974102020264\n",
            "Epoch 1/70\n",
            "72/72 [==============================] - 8s 72ms/step - loss: 3.6840 - val_loss: 3.2394\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 4s 40ms/step - loss: 2.9546 - val_loss: 2.7082\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.6951 - val_loss: 2.5772\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 2.5994 - val_loss: 2.5100\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 3s 25ms/step - loss: 2.5379 - val_loss: 2.4614\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 3s 15ms/step - loss: 2.4885 - val_loss: 2.4220\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.4454 - val_loss: 2.3849\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.4063 - val_loss: 2.3435\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.3688 - val_loss: 2.3051\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.3360 - val_loss: 2.2713\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.3019 - val_loss: 2.2377\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 4s 24ms/step - loss: 2.2710 - val_loss: 2.2085\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.2411 - val_loss: 2.1839\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.2135 - val_loss: 2.1584\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.1844 - val_loss: 2.1328\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.1586 - val_loss: 2.1046\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.1367 - val_loss: 2.0818\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 2.1121 - val_loss: 2.0629\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.0906 - val_loss: 2.0457\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0729 - val_loss: 2.0270\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0545 - val_loss: 2.0157\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.0378 - val_loss: 1.9999\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 2.0223 - val_loss: 1.9878\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.0064 - val_loss: 1.9784\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9934 - val_loss: 1.9661\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.9816 - val_loss: 1.9529\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.9693 - val_loss: 1.9428\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9578 - val_loss: 1.9342\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9471 - val_loss: 1.9270\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.9373 - val_loss: 1.9243\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9278 - val_loss: 1.9165\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 3s 25ms/step - loss: 1.9182 - val_loss: 1.9090\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.9087 - val_loss: 1.9032\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9004 - val_loss: 1.8992\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8917 - val_loss: 1.8987\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.8847 - val_loss: 1.8894\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.8782 - val_loss: 1.8821\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.8715 - val_loss: 1.8766\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8651 - val_loss: 1.8751\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8581 - val_loss: 1.8721\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8524 - val_loss: 1.8678\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 4s 26ms/step - loss: 1.8461 - val_loss: 1.8674\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8404 - val_loss: 1.8630\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8343 - val_loss: 1.8566\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8305 - val_loss: 1.8543\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.8240 - val_loss: 1.8506\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.8196 - val_loss: 1.8505\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8146 - val_loss: 1.8504\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8106 - val_loss: 1.8471\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8062 - val_loss: 1.8385\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.8026 - val_loss: 1.8421\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7972 - val_loss: 1.8436\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.7938 - val_loss: 1.8400\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7893 - val_loss: 1.8365\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7855 - val_loss: 1.8337\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.7817 - val_loss: 1.8396\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 3s 22ms/step - loss: 1.7769 - val_loss: 1.8345\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7740 - val_loss: 1.8372\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.7698 - val_loss: 1.8303\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.7677 - val_loss: 1.8298\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.7629 - val_loss: 1.8296\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.7602 - val_loss: 1.8245\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.7587 - val_loss: 1.8234\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7544 - val_loss: 1.8234\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.7514 - val_loss: 1.8192\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7479 - val_loss: 1.8261\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7455 - val_loss: 1.8250\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 3s 21ms/step - loss: 1.7418 - val_loss: 1.8236\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 3s 15ms/step - loss: 1.7401 - val_loss: 1.8217\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.7374 - val_loss: 1.8213\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.8024\n",
            "eval_loss: 1.80240797996521\n",
            "perplexity 6.064232447429861\n",
            "\n",
            "Оматse\n",
            "Илет яя дети,\n",
            "Озавый —\n",
            "Гивиск – м;\n",
            "И вомовом ст —\n",
            "Погой вой лямота обетути нистум сахокалугдрашелой ся ла скалем, сть ойсалимоски мегода ужи дех\n",
            "И нем за в ивых вой на прера меть четы встылутпа\n",
            "Илюнеле вом погратой:\n",
            "Оти мой водолитося, недода спи во учь сча нилоковой скогрой прасты,\n",
            "Гонотозам рой уюдо и ми,\n",
            "Авезасямогой иць м сь м?\n",
            "Вый вобой:\n",
            "И боза,\n",
            "Да – ствикощерисашананед ламуска уднать мены\n",
            "Дрогокуставетола илогли д летань гда дапо у и нав ст и в ноитнь прогрза стра слицатонедежет ной рбе сторь.\n",
            "Оламоть вал ветогре помозоспрутылалочищима нетый нотишиет счана-бута\n",
            "Бе тимопо сла вочистихозвста у – мритегожи…\n",
            "Та ве,\n",
            "Ба та намегренететю жднь;\n",
            "Вскалетавою стслам телезмотав, ный теста углень ста морубилет'Тодетететой дори ми по бой,\n",
            "Вой неглеми тазый традналетух бо до поше ск ся и лучуго стегробегора биво пи по:\n",
            "Прах т жи налодаль четой пеност,\n",
            "Стя мощей дром плелигой нетый стестием жи вегодрами ве сту,\n",
            "И пой бв вокоть нера мода нь пой пра стнь их и ный;\n",
            "Вовена деменеромошагдоме т\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.835563898086548\n",
            "Epoch 1/70\n",
            "72/72 [==============================] - 9s 92ms/step - loss: 3.6028 - val_loss: 2.9587\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 2.7836 - val_loss: 2.6166\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 2.6136 - val_loss: 2.5266\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.5348 - val_loss: 2.4567\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 4s 47ms/step - loss: 2.4679 - val_loss: 2.4110\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.4150 - val_loss: 2.3613\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.3721 - val_loss: 2.3120\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.3296 - val_loss: 2.2773\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.2893 - val_loss: 2.2387\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.2529 - val_loss: 2.2031\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 2.2162 - val_loss: 2.1684\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.1820 - val_loss: 2.1377\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.1501 - val_loss: 2.1064\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.1192 - val_loss: 2.0798\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.0910 - val_loss: 2.0584\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 4s 19ms/step - loss: 2.0644 - val_loss: 2.0322\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 2.0405 - val_loss: 2.0089\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.0187 - val_loss: 1.9970\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9980 - val_loss: 1.9795\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9792 - val_loss: 1.9651\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.9613 - val_loss: 1.9495\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9455 - val_loss: 1.9370\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.9300 - val_loss: 1.9283\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.9168 - val_loss: 1.9160\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9042 - val_loss: 1.9059\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 4s 19ms/step - loss: 1.8904 - val_loss: 1.8984\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.8791 - val_loss: 1.8922\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.8679 - val_loss: 1.8845\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 3s 24ms/step - loss: 1.8572 - val_loss: 1.8802\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 4s 22ms/step - loss: 1.8484 - val_loss: 1.8753\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.8381 - val_loss: 1.8618\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.8299 - val_loss: 1.8587\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8209 - val_loss: 1.8530\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.8128 - val_loss: 1.8505\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.8050 - val_loss: 1.8467\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.7989 - val_loss: 1.8361\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7905 - val_loss: 1.8397\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7830 - val_loss: 1.8389\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7770 - val_loss: 1.8322\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7710 - val_loss: 1.8280\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.7623 - val_loss: 1.8317\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.7581 - val_loss: 1.8237\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7528 - val_loss: 1.8243\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7471 - val_loss: 1.8164\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7420 - val_loss: 1.8199\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7367 - val_loss: 1.8179\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7310 - val_loss: 1.8162\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.7273 - val_loss: 1.8148\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.7216 - val_loss: 1.8124\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7174 - val_loss: 1.8123\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7123 - val_loss: 1.8149\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7095 - val_loss: 1.8100\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.7043 - val_loss: 1.8085\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.6998 - val_loss: 1.8103\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6961 - val_loss: 1.8050\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6931 - val_loss: 1.8047\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6880 - val_loss: 1.8068\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6846 - val_loss: 1.8071\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6808 - val_loss: 1.8061\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.6764 - val_loss: 1.8036\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6750 - val_loss: 1.8056\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6701 - val_loss: 1.8030\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6665 - val_loss: 1.8044\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6633 - val_loss: 1.8074\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.6604 - val_loss: 1.8007\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6575 - val_loss: 1.8038\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6545 - val_loss: 1.8061\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6507 - val_loss: 1.8029\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6481 - val_loss: 1.8055\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6452 - val_loss: 1.8073\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.7772\n",
            "eval_loss: 1.777166724205017\n",
            "perplexity 5.913079278288531\n",
            "\n",
            "Ностени сихоре,\n",
            "\n",
            "Ссты сточеголой по гой в невара,\n",
            "Потрнай встоговай ст в потост.\n",
            "И на по ве ре бу пож м и</s сти модедатой нолит пе оля васть вдамем\"Атат в кланита сль ука\n",
            "В ст,\n",
            "Тостедей стено см; пе ского нилы вовь вачискатола пот.\n",
            "И стодни дый угони вь вов ст ла Тона>\n",
            "Грогутый вежи ста!\n",
            "Ве пезани погуждеза, да спося;\n",
            "О в бы\n",
            "Ме ско вроговыде ски ся в вь т гох ститам пою молаза ви мей вся,\n",
            "Пося\n",
            "А щилетразной де, ве м сь мскретама кагли де я ва изашужубосв пом ве ноцатый\n",
            "Ве и по ра на\n",
            "Вероюнень бо по полотво ве ни сти нот,\n",
            "Заesсть ока деу,\n",
            "«Мизось нукитый;\n",
            "Да зый вивом уполай сти,\n",
            "Ста и м скоподо повой нила ко ив м.\n",
            "</s гот.\n",
            "Опотысте кановни ст нашлона нум овица воты вость на мой но с,\n",
            "Еся Гле всти вамчаруднона,\n",
            "И мужи итв ночудеза пя постове вети дабла звою стой сяладудасто м о у\n",
            "Хосе,\n",
            "Ила оли мочишет,\n",
            "\n",
            "Споба вот.\n",
            "\n",
            "Поста дуповенедатом\n",
            "Уни го Говоюбре ую;\n",
            "Ста о скасть во жизда мизый сть по пой нети ста натой;\n",
            "И воскою ле вив в стра ра сти ов м валою пост ся;\n",
            "Евногон чина по и столый пол\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1312079429626465\n",
            "Epoch 1/70\n",
            "72/72 [==============================] - 8s 75ms/step - loss: 3.4032 - val_loss: 2.7451\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 4s 40ms/step - loss: 2.6504 - val_loss: 2.5359\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.5290 - val_loss: 2.4565\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.4480 - val_loss: 2.3813\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.3761 - val_loss: 2.3143\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.3158 - val_loss: 2.2634\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 2.2629 - val_loss: 2.2205\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.2145 - val_loss: 2.1757\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.1705 - val_loss: 2.1300\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.1281 - val_loss: 2.0906\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.0903 - val_loss: 2.0614\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.0549 - val_loss: 2.0302\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 2.0231 - val_loss: 2.0129\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.9943 - val_loss: 1.9844\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9675 - val_loss: 1.9699\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9470 - val_loss: 1.9446\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.9247 - val_loss: 1.9306\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 3s 24ms/step - loss: 1.9064 - val_loss: 1.9151\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8884 - val_loss: 1.9073\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8724 - val_loss: 1.8895\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.8567 - val_loss: 1.8848\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8434 - val_loss: 1.8749\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.8312 - val_loss: 1.8704\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 3s 23ms/step - loss: 1.8186 - val_loss: 1.8664\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8071 - val_loss: 1.8564\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7962 - val_loss: 1.8492\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7872 - val_loss: 1.8489\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7775 - val_loss: 1.8385\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.7686 - val_loss: 1.8380\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.7601 - val_loss: 1.8284\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7519 - val_loss: 1.8283\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7431 - val_loss: 1.8245\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7363 - val_loss: 1.8200\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7286 - val_loss: 1.8254\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.7228 - val_loss: 1.8183\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.7155 - val_loss: 1.8114\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7087 - val_loss: 1.8122\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7037 - val_loss: 1.8136\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.6966 - val_loss: 1.8068\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.6905 - val_loss: 1.8047\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6855 - val_loss: 1.8068\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6812 - val_loss: 1.8085\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6746 - val_loss: 1.8048\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6694 - val_loss: 1.8016\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6640 - val_loss: 1.8001\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.6604 - val_loss: 1.8016\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6552 - val_loss: 1.8042\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6512 - val_loss: 1.8022\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6469 - val_loss: 1.8022\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6416 - val_loss: 1.8012\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6380 - val_loss: 1.7977\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.6340 - val_loss: 1.8005\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6293 - val_loss: 1.7982\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6255 - val_loss: 1.7962\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6218 - val_loss: 1.8004\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6200 - val_loss: 1.8014\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6146 - val_loss: 1.7998\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.6119 - val_loss: 1.8003\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6079 - val_loss: 1.8052\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6037 - val_loss: 1.8050\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.5999 - val_loss: 1.8038\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5984 - val_loss: 1.8039\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5937 - val_loss: 1.8043\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.5918 - val_loss: 1.8039\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.5884 - val_loss: 1.8041\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5851 - val_loss: 1.8029\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5837 - val_loss: 1.8038\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5791 - val_loss: 1.8040\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5769 - val_loss: 1.8060\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.5735 - val_loss: 1.8052\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.7788\n",
            "eval_loss: 1.778821349143982\n",
            "perplexity 5.922871305560689\n",
            "\n",
            "И па ме скра мытратый,\n",
            "Ной ты езала мо све икроеренсм?,\n",
            "Омугостой,\n",
            "Аодной\n",
            "Нанавый но серой воро ви вы!\n",
            "Ум,\n",
            "Бе,\n",
            "Мо детрат зобой!\n",
            "Огола мой мета – дых тив сть pon спрениза по дестьми зодено,\n",
            "Виздит бо пежевеносоронебредеднеза гой ни ся, вужерастодедолако м годь вилуто лола таг ла зной сля.\n",
            "А ся вету, и вала ст здлит ви\n",
            "Пик ут иволитой стой —\n",
            "Глаль еволиве вет зам:\n",
            "И ирпой породо\n",
            "Пре,\n",
            "\n",
            "В праль но – стыст,\n",
            "Дидропрелелаварум статотода сугой норою м гой ветом дуголось дудилово годь стьедет рениви вы сежуня, наей намачитцашазый бробрди\n",
            "Тытой пе то ни мала,\n",
            "Хора ско зит но и стобракужилох гой бересяный,\n",
            "Стум?\n",
            "И кане нем ся ноко ка битли,\n",
            "А вены меси педра лё поба ся повоби, зомедамещезнай.\n",
            "Хлотый в до вни саседо ой залесть жиздрав сть мо но иценеме пра би с.\n",
            "Ожди микогутосли пой и ти,\n",
            "Луше, Гистри лотнасчеднь пушко нь ненle продни Мо вена,\n",
            "Витой то издивобрака ти ной, санива ноTon стой ила еном звею моди гока во, во л,\n",
            "Ты стумешугок сы мотовелой,\n",
            "Лю этчи сененодла м х я свещежеслена,\n",
            "Уный\n",
            "Гиле\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4088122844696045\n",
            "Epoch 1/70\n",
            "72/72 [==============================] - 10s 87ms/step - loss: 3.2872 - val_loss: 2.6619\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 4s 32ms/step - loss: 2.5828 - val_loss: 2.4847\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.4624 - val_loss: 2.3980\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 3s 25ms/step - loss: 2.3807 - val_loss: 2.3213\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 2.3021 - val_loss: 2.2473\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 3s 25ms/step - loss: 2.2328 - val_loss: 2.1913\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.1736 - val_loss: 2.1423\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.1224 - val_loss: 2.0964\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 2.0752 - val_loss: 2.0579\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 2.0345 - val_loss: 2.0220\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 3s 23ms/step - loss: 1.9984 - val_loss: 1.9940\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.9663 - val_loss: 1.9661\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.9373 - val_loss: 1.9471\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.9120 - val_loss: 1.9247\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.8894 - val_loss: 1.9120\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.8675 - val_loss: 1.8916\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.8493 - val_loss: 1.8842\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.8319 - val_loss: 1.8670\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.8160 - val_loss: 1.8592\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.8018 - val_loss: 1.8540\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.7863 - val_loss: 1.8403\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 3s 22ms/step - loss: 1.7747 - val_loss: 1.8357\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.7633 - val_loss: 1.8315\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.7514 - val_loss: 1.8263\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.7406 - val_loss: 1.8216\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.7306 - val_loss: 1.8125\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.7203 - val_loss: 1.8082\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 3s 21ms/step - loss: 1.7116 - val_loss: 1.8070\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 3s 24ms/step - loss: 1.7037 - val_loss: 1.8072\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6942 - val_loss: 1.8008\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6866 - val_loss: 1.7966\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6792 - val_loss: 1.7964\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.6712 - val_loss: 1.7951\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 3s 21ms/step - loss: 1.6640 - val_loss: 1.7947\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6566 - val_loss: 1.7923\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.6513 - val_loss: 1.7919\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6437 - val_loss: 1.7926\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6386 - val_loss: 1.7892\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 3s 23ms/step - loss: 1.6323 - val_loss: 1.7859\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.6263 - val_loss: 1.7909\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6206 - val_loss: 1.7918\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6142 - val_loss: 1.7871\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6101 - val_loss: 1.7908\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.6053 - val_loss: 1.7913\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 3s 21ms/step - loss: 1.5993 - val_loss: 1.7889\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5959 - val_loss: 1.7894\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5896 - val_loss: 1.7904\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5838 - val_loss: 1.7912\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5806 - val_loss: 1.7935\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.5756 - val_loss: 1.7937\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 3s 21ms/step - loss: 1.5718 - val_loss: 1.7910\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.5669 - val_loss: 1.7963\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5638 - val_loss: 1.7968\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5589 - val_loss: 1.7928\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 3s 21ms/step - loss: 1.5541 - val_loss: 1.7951\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.5501 - val_loss: 1.7982\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 2s 19ms/step - loss: 1.5466 - val_loss: 1.7995\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5433 - val_loss: 1.8025\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.5391 - val_loss: 1.8031\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 3s 22ms/step - loss: 1.5360 - val_loss: 1.8008\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.5317 - val_loss: 1.8018\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.5290 - val_loss: 1.8008\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5249 - val_loss: 1.7964\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 2s 22ms/step - loss: 1.5221 - val_loss: 1.8088\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 3s 23ms/step - loss: 1.5193 - val_loss: 1.8069\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 1.5154 - val_loss: 1.8074\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5125 - val_loss: 1.8098\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.5094 - val_loss: 1.8090\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 3s 22ms/step - loss: 1.5066 - val_loss: 1.8122\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.5039 - val_loss: 1.8149\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 1.7844\n",
            "eval_loss: 1.7843518257141113\n",
            "perplexity 5.955718352734306\n",
            "\n",
            "Ву м\n",
            "Пря м пленедем угде Токумегора,\n",
            "Ну налови угренастро ной т рой расый.\n",
            "Какреве т ил, как,\n",
            "То мала сту ждони\n",
            "Пиздобы.\n",
            "И тамой,\n",
            "И неномеро зилак пукрониней микаю ся пра ло стамиздах дадоюбугоски н в за пра,\n",
            "\n",
            "И жедасло пра ило побяни негом ой сть стра и гой с осаюбеже о ост ст, нашите ть де мо сь сх зави ть, талав татьютовитастра в спры;\n",
            "Уешь побой,\n",
            "Он,\n",
            "Оной;\n",
            "Вети мутвелетыла м:\n",
            "Стра стах\n",
            "Чте нистуме чезо, поволо и узо тв ла,\n",
            "Ты\n",
            "Прая за </s>\n",
            "Касстуго о х погомезадасто детя!.\n",
            "Модосто уянера сунодрой;\n",
            "Тый;\n",
            "Нья! и убаре в вем ганоре на лирахоса,\n",
            "И про за преривежи прый, мобеспраканогой:\n",
            "И сту ска стом\n",
            "Нолитеза по повья слакосни ждетугостовобенамаве у веноль ра пре вой назала ся,\n",
            "Чт буго ност и м ся моглурах нет порати ной,\n",
            "И,\n",
            "За,\n",
            "Ве вай.\n",
            "Нашет ти, нашел тнь сь,\n",
            "Бугобый м ск ролало ся\n",
            "Венак меде бестою нок сть;\n",
            "Ти,\n",
            "От стогоскушь орода,\n",
            "Пра детря бо заспо ракана повенове зый в ся пра к ви влак пей.\n",
            "Ви, зновеньсчеродань нобаретазах мой бежили,\n",
            "Ках ой стоюбе вилены, мочах ра стро поме слыто \n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.831484079360962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_embedding_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zoVJ9a9P8XN8",
        "outputId": "3c23aa2e-f0c6-44d1-a70d-9fe4933e5fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  eval_loss perplexity  \\\n",
              "embedding_dim 35   1.842625   6.313088   \n",
              "embedding_dim 71   1.802408   6.064232   \n",
              "embedding_dim 143  1.777167   5.913079   \n",
              "embedding_dim 286  1.778821   5.922871   \n",
              "embedding_dim 572  1.784352   5.955718   \n",
              "\n",
              "                                                         result_text  run_time  \n",
              "embedding_dim 35   \\nОгезродвопо нь стакой ва коме па бе звсть ни...  2.883097  \n",
              "embedding_dim 71   \\nОматse\\nИлет яя дети,\\nОзавый —\\nГивиск – м;...  2.835564  \n",
              "embedding_dim 143  \\nНостени сихоре,\\n\\nСсты сточеголой по гой в ...  3.131208  \n",
              "embedding_dim 286  \\nИ па ме скра мытратый,\\nНой ты езала мо све ...  3.408812  \n",
              "embedding_dim 572  \\nВу м\\nПря м пленедем угде Токумегора,\\nНу на...  2.831484  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2f26d74-4157-46a2-a165-8f2de311836c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eval_loss</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>result_text</th>\n",
              "      <th>run_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>embedding_dim 35</th>\n",
              "      <td>1.842625</td>\n",
              "      <td>6.313088</td>\n",
              "      <td>\\nОгезродвопо нь стакой ва коме па бе звсть ни...</td>\n",
              "      <td>2.883097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embedding_dim 71</th>\n",
              "      <td>1.802408</td>\n",
              "      <td>6.064232</td>\n",
              "      <td>\\nОматse\\nИлет яя дети,\\nОзавый —\\nГивиск – м;...</td>\n",
              "      <td>2.835564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embedding_dim 143</th>\n",
              "      <td>1.777167</td>\n",
              "      <td>5.913079</td>\n",
              "      <td>\\nНостени сихоре,\\n\\nСсты сточеголой по гой в ...</td>\n",
              "      <td>3.131208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embedding_dim 286</th>\n",
              "      <td>1.778821</td>\n",
              "      <td>5.922871</td>\n",
              "      <td>\\nИ па ме скра мытратый,\\nНой ты езала мо све ...</td>\n",
              "      <td>3.408812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embedding_dim 572</th>\n",
              "      <td>1.784352</td>\n",
              "      <td>5.955718</td>\n",
              "      <td>\\nВу м\\nПря м пленедем угде Токумегора,\\nНу на...</td>\n",
              "      <td>2.831484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2f26d74-4157-46a2-a165-8f2de311836c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2f26d74-4157-46a2-a165-8f2de311836c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2f26d74-4157-46a2-a165-8f2de311836c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in dict_embedding_dim_describe.items():\n",
        "  print(key, value, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZZ1fBo58XpV",
        "outputId": "00dcf420-9ef3-402c-e5ab-717d840da4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding_dim 35\n",
            "              len      lines  mean_line_len\n",
            "count    2.000000   2.000000       2.000000\n",
            "mean   497.000000  14.500000      33.077778\n",
            "std    275.771645   7.778175       0.738534\n",
            "min    302.000000   9.000000      32.555556\n",
            "25%    399.500000  11.750000      32.816667\n",
            "50%    497.000000  14.500000      33.077778\n",
            "75%    594.500000  17.250000      33.338889\n",
            "max    692.000000  20.000000      33.600000\n",
            "embedding_dim 71\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0           1.00\n",
            "mean   1000.0   24.0          39.04\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0   24.0          39.04\n",
            "25%    1000.0   24.0          39.04\n",
            "50%    1000.0   24.0          39.04\n",
            "75%    1000.0   24.0          39.04\n",
            "max    1000.0   24.0          39.04\n",
            "embedding_dim 143\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0       1.000000\n",
            "mean   1000.0   33.0      31.193548\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0   33.0      31.193548\n",
            "25%    1000.0   33.0      31.193548\n",
            "50%    1000.0   33.0      31.193548\n",
            "75%    1000.0   33.0      31.193548\n",
            "max    1000.0   33.0      31.193548\n",
            "embedding_dim 286\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0       1.000000\n",
            "mean   1000.0   30.0      32.333333\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0   30.0      32.333333\n",
            "25%    1000.0   30.0      32.333333\n",
            "50%    1000.0   30.0      32.333333\n",
            "75%    1000.0   30.0      32.333333\n",
            "max    1000.0   30.0      32.333333\n",
            "embedding_dim 572\n",
            "              len      lines  mean_line_len\n",
            "count    2.000000   2.000000       2.000000\n",
            "mean   496.500000  17.500000      26.523220\n",
            "std     78.488853   0.707107       2.237347\n",
            "min    441.000000  17.000000      24.941176\n",
            "25%    468.750000  17.250000      25.732198\n",
            "50%    496.500000  17.500000      26.523220\n",
            "75%    524.250000  17.750000      27.314241\n",
            "max    552.000000  18.000000      28.105263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При увеличении размеров эмбеддингов перплексия и ошибка на тестовых данных уменьшались, улучшалось качество текстов, однако при размерности эмбеддинга 572 результаты стали хуже, что скорее всего свидетельствует о переобучении. Оптимальное значение параметра embedding dim - 286."
      ],
      "metadata": {
        "id": "WR1YjFq5DnSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "gru_units = [10, 100, 300, 500]\n",
        "T = 0.7\n",
        "NUM_EPOCHS=70"
      ],
      "metadata": {
        "id": "7QBgi98rF85O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gru_units = pd.DataFrame(\n",
        "    columns=['eval_loss', 'perplexity', 'result_text', 'run_time'], \n",
        "    index=['gru_units 10', 'gru_units 100', 'gru_units 300', 'gru_units 500'])\n",
        "dict_gru_units_describe = {}"
      ],
      "metadata": {
        "id": "kbSflyZCGcUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in gru_units:\n",
        "    model = MyModel(\n",
        "        vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "        embedding_dim=embedding_dim,\n",
        "        gru_units=i)\n",
        "    model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset, \n",
        "        validation_data = val_dataset, \n",
        "        epochs=NUM_EPOCHS, \n",
        "        callbacks=[checkpoint_callback])\n",
        "    \n",
        "    eval_loss = model.evaluate(test_dataset)\n",
        "    perplexity = np.exp(eval_loss)\n",
        "    print('eval_loss:', eval_loss)\n",
        "    print('perplexity', perplexity)\n",
        "\n",
        "    one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "    start = time.time()\n",
        "    states = None\n",
        "    next_char = tf.constant(['\\n'])\n",
        "    result = [next_char]\n",
        "\n",
        "    for n in range(N):\n",
        "        next_char, states = one_step_model.generate_one_step(next_char, states=states, temperature=T)\n",
        "        result.append(next_char)\n",
        "\n",
        "    result = tf.strings.join(result)\n",
        "    end = time.time()\n",
        "\n",
        "    result_text = result[0].numpy().decode('utf-8')\n",
        "    print(result_text)\n",
        "    print('_'*80)\n",
        "    run_time = end - start\n",
        "    print('\\nRun time:', run_time)\n",
        "\n",
        "    df_gru_units.loc[f'gru_units {i}'] = [eval_loss, perplexity, result_text, run_time]\n",
        "    dict_gru_units_describe[f'gru_units {i}'] = describe_poems(result_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Zfk74yG8k1",
        "outputId": "5a8c52fb-5c48-4b86-c61d-785d76911347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "72/72 [==============================] - 8s 79ms/step - loss: 4.5247 - val_loss: 3.9357\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 2s 21ms/step - loss: 3.7057 - val_loss: 3.5445\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 3.4827 - val_loss: 3.4136\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 3.3963 - val_loss: 3.3537\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 2s 10ms/step - loss: 3.3425 - val_loss: 3.2995\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 3.2827 - val_loss: 3.2182\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 3.1777 - val_loss: 3.1129\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 3.0963 - val_loss: 3.0544\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 3.0431 - val_loss: 3.0050\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.9996 - val_loss: 2.9618\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.9551 - val_loss: 2.9139\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.9086 - val_loss: 2.8722\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.8700 - val_loss: 2.8339\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 1s 10ms/step - loss: 2.8327 - val_loss: 2.7973\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.7992 - val_loss: 2.7682\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.7721 - val_loss: 2.7440\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.7487 - val_loss: 2.7212\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.7271 - val_loss: 2.7009\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.7095 - val_loss: 2.6865\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.6959 - val_loss: 2.6750\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.6822 - val_loss: 2.6621\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.6696 - val_loss: 2.6467\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.6579 - val_loss: 2.6379\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.6473 - val_loss: 2.6282\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.6383 - val_loss: 2.6208\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 1s 7ms/step - loss: 2.6302 - val_loss: 2.6126\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 2.6226 - val_loss: 2.6051\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 2s 9ms/step - loss: 2.6157 - val_loss: 2.5985\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.6089 - val_loss: 2.5944\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.6033 - val_loss: 2.5858\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5971 - val_loss: 2.5800\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 2s 10ms/step - loss: 2.5911 - val_loss: 2.5738\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.5856 - val_loss: 2.5706\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.5801 - val_loss: 2.5630\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 2s 10ms/step - loss: 2.5753 - val_loss: 2.5592\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 1s 7ms/step - loss: 2.5707 - val_loss: 2.5543\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5666 - val_loss: 2.5514\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.5623 - val_loss: 2.5485\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.5589 - val_loss: 2.5435\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.5553 - val_loss: 2.5407\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 4s 14ms/step - loss: 2.5524 - val_loss: 2.5381\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 2s 8ms/step - loss: 2.5486 - val_loss: 2.5345\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 1s 7ms/step - loss: 2.5459 - val_loss: 2.5297\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5421 - val_loss: 2.5264\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5392 - val_loss: 2.5230\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.5362 - val_loss: 2.5201\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.5333 - val_loss: 2.5182\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 3s 10ms/step - loss: 2.5307 - val_loss: 2.5154\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5281 - val_loss: 2.5131\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5252 - val_loss: 2.5112\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5227 - val_loss: 2.5071\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.5203 - val_loss: 2.5041\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5182 - val_loss: 2.5023\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 2s 12ms/step - loss: 2.5157 - val_loss: 2.4990\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.5140 - val_loss: 2.4988\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 1s 7ms/step - loss: 2.5113 - val_loss: 2.4968\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.5096 - val_loss: 2.4939\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 1s 7ms/step - loss: 2.5077 - val_loss: 2.4939\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.5055 - val_loss: 2.4924\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.5036 - val_loss: 2.4897\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 12ms/step - loss: 2.5017 - val_loss: 2.4869\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 2s 10ms/step - loss: 2.5005 - val_loss: 2.4855\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.4983 - val_loss: 2.4845\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.4967 - val_loss: 2.4823\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 2.4951 - val_loss: 2.4804\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.4930 - val_loss: 2.4791\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 2s 12ms/step - loss: 2.4916 - val_loss: 2.4774\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.4900 - val_loss: 2.4766\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 2s 8ms/step - loss: 2.4887 - val_loss: 2.4736\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.4869 - val_loss: 2.4733\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 2.4727\n",
            "eval_loss: 2.4727134704589844\n",
            "perplexity 11.854570275201459\n",
            "\n",
            "Ра ра, бу ут,\n",
            "Кой т.\n",
            "\n",
            "Нацака ис ся ны дья боти ра наких оста ми\n",
            "Пе жн, ж и в исю посе е ме ти, кора пог, и\n",
            "За добох узши, м и бя дась.\n",
            "Бе дола гню,\n",
            "На\n",
            "Сти вость,\n",
            "Ке хое ме, ны м са ты ны в вня ук па, то ве зоми ти ны у,\n",
            "Ле сты возя костдася ст\n",
            "Не высть пи ной в гою кой.\n",
            "Вста нерогрезумо ре би поще же эть и я ихи –\n",
            "Пы ведодаме павой вль, ены пакою зце тьсте рыми ныви коте гны бимо ка го ных, рым,\n",
            "Пе о ми тви мы мы пы да то нымыши подомою де орара ны ви окок устчт; кли ре вы мизи нь ве бы дой, рах ра скуки.\n",
            "И по ты ка.\n",
            "Кадоебебости стне\n",
            "И о мы ты пози, ле па,\n",
            "На, в па, ве ушал зи, педены, вы доносье поля летое ка, Водай жь коля тет…\n",
            "Вы ст\n",
            "Налысто по мивистаклих по чай вы ня каканы де чашя ули я мы зы и ости ви,\n",
            "Прай;\n",
            "По вобя повов.\n",
            "Чя – тихаха ды ви пору? пе те пу пенозуфо о е гно побе ть у к Бе помой о тены ка комо,\n",
            "Я го пощератой ло\n",
            "Его сасе к скимруи те да кувенымочасе не Госа ли лати стас ком таклулисти и мы ны бо дут.\n",
            "И оде шетеска по до сты в Ме, ка патыхой ви Дра вы не му ска пи т\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.813267707824707\n",
            "Epoch 1/70\n",
            "72/72 [==============================] - 9s 79ms/step - loss: 3.7768 - val_loss: 3.1851\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 2.9201 - val_loss: 2.7282\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.6843 - val_loss: 2.5932\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 2s 12ms/step - loss: 2.5794 - val_loss: 2.5136\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.5043 - val_loss: 2.4494\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.4470 - val_loss: 2.4060\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.4067 - val_loss: 2.3705\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.3734 - val_loss: 2.3370\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.3446 - val_loss: 2.3133\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.3188 - val_loss: 2.2867\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.2965 - val_loss: 2.2658\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.2755 - val_loss: 2.2469\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.2570 - val_loss: 2.2275\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 2s 10ms/step - loss: 2.2396 - val_loss: 2.2134\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.2231 - val_loss: 2.1974\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.2085 - val_loss: 2.1851\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 2s 12ms/step - loss: 2.1948 - val_loss: 2.1750\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.1813 - val_loss: 2.1578\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 9ms/step - loss: 2.1685 - val_loss: 2.1461\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.1572 - val_loss: 2.1326\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.1464 - val_loss: 2.1260\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.1366 - val_loss: 2.1170\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.1280 - val_loss: 2.1094\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 2s 12ms/step - loss: 2.1183 - val_loss: 2.1000\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.1106 - val_loss: 2.0918\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.1033 - val_loss: 2.0852\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.0952 - val_loss: 2.0796\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0874 - val_loss: 2.0724\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0814 - val_loss: 2.0645\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0750 - val_loss: 2.0624\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0686 - val_loss: 2.0564\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 2s 9ms/step - loss: 2.0631 - val_loss: 2.0514\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0575 - val_loss: 2.0459\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0520 - val_loss: 2.0398\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0468 - val_loss: 2.0388\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.0413 - val_loss: 2.0303\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0374 - val_loss: 2.0228\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 2.0327 - val_loss: 2.0254\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0283 - val_loss: 2.0156\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0240 - val_loss: 2.0156\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.0199 - val_loss: 2.0132\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 2.0151 - val_loss: 2.0073\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0122 - val_loss: 2.0071\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 2s 12ms/step - loss: 2.0085 - val_loss: 2.0089\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 2.0052 - val_loss: 2.0014\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 2.0008 - val_loss: 1.9975\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9974 - val_loss: 1.9917\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9949 - val_loss: 1.9861\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9919 - val_loss: 1.9905\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 2s 14ms/step - loss: 1.9895 - val_loss: 1.9834\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 3s 14ms/step - loss: 1.9854 - val_loss: 1.9863\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9828 - val_loss: 1.9842\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9796 - val_loss: 1.9797\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9776 - val_loss: 1.9788\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9756 - val_loss: 1.9778\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9729 - val_loss: 1.9720\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 1.9701 - val_loss: 1.9729\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9679 - val_loss: 1.9698\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9649 - val_loss: 1.9688\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9639 - val_loss: 1.9659\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9608 - val_loss: 1.9647\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9584 - val_loss: 1.9654\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9564 - val_loss: 1.9622\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9545 - val_loss: 1.9642\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9525 - val_loss: 1.9551\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 2s 9ms/step - loss: 1.9505 - val_loss: 1.9558\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 2s 13ms/step - loss: 1.9485 - val_loss: 1.9542\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 1.9477 - val_loss: 1.9501\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 2s 11ms/step - loss: 1.9444 - val_loss: 1.9517\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 2s 12ms/step - loss: 1.9433 - val_loss: 1.9521\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 1.9383\n",
            "eval_loss: 1.938313364982605\n",
            "perplexity 6.947023989921041\n",
            "\n",
            "THiex помпося го\n",
            "Бося когл!.\n",
            "Биходнь, сю да би ко бак ск сас пною,\n",
            "Дамуть дая том бо пове велстешь стимиск вебыл!\n",
            "Ей\n",
            "Нейневой,\n",
            "На,\n",
            "Уя и я ли галы том оя зе, вогломом сь вет и вогнымих пой ся вына-ва дам,\n",
            "Сат на дося вой, вете ст<>\n",
            "Вамой ст бе,\n",
            "При ся ми ластвезнь ито?\n",
            "Ть,\n",
            "Чи нилежда нгдыли ст ед т,\n",
            "\n",
            " —\n",
            "\n",
            "Вой,\n",
            "Кранякать:\n",
            "\n",
            "Ле ветвовой с,\n",
            "\n",
            "Ст ме патасую поцанисью сто пинидямороми шинесаска зневы,\n",
            "Увело томели вогломи Момщи гото ньеносласпой – т.\n",
            "Му, ст сток т\n",
            "И сугомл:\n",
            "…\n",
            "Вобуть ва постая лчей злогодули пикодустом во да нелеменомутожеся новемучаногорА во сю </so: ся тост ск менела дани дя смгом стогой бый,\n",
            "Чара ит вели комутостазласпошью, ся гоедамераволесёпеботой: вероститрася ветоно да быкрая сть т,\n",
            "\"улем сли мугой и\n",
            "\n",
            "Не, ик.\n",
            "И пова по сю праветь смолю,\n",
            "Я телоглитыл,\n",
            "Не волиредая накакочазнамылодь дадося носе.\n",
            "Дая мы ск помутовый мула но мид гомногогок натя бый, Ств ть сьм вда веска вотой </s имемутасмра э>\n",
            "Вь дя ны рай.\n",
            "\n",
            "Ност гдучодам.\n",
            "Уст пр и здой, издериценамуй погнатама скогли сvetex\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5764989852905273\n",
            "Epoch 1/70\n",
            "72/72 [==============================] - 8s 67ms/step - loss: 3.4369 - val_loss: 2.7667\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 4s 39ms/step - loss: 2.6635 - val_loss: 2.5439\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 2.5365 - val_loss: 2.4565\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 2.4512 - val_loss: 2.3831\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.3820 - val_loss: 2.3212\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 3s 21ms/step - loss: 2.3251 - val_loss: 2.2756\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.2745 - val_loss: 2.2317\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.2269 - val_loss: 2.1809\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.1834 - val_loss: 2.1416\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 2.1421 - val_loss: 2.1093\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.1037 - val_loss: 2.0763\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 2.0690 - val_loss: 2.0488\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0364 - val_loss: 2.0179\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 2.0074 - val_loss: 1.9930\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9806 - val_loss: 1.9749\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9575 - val_loss: 1.9562\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9354 - val_loss: 1.9435\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.9171 - val_loss: 1.9220\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.9002 - val_loss: 1.9121\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.8833 - val_loss: 1.8981\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8678 - val_loss: 1.8870\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.8540 - val_loss: 1.8785\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.8407 - val_loss: 1.8712\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.8285 - val_loss: 1.8609\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.8172 - val_loss: 1.8516\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.8054 - val_loss: 1.8504\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7955 - val_loss: 1.8498\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7864 - val_loss: 1.8447\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.7776 - val_loss: 1.8342\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 3s 16ms/step - loss: 1.7690 - val_loss: 1.8334\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.7603 - val_loss: 1.8260\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7517 - val_loss: 1.8265\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7452 - val_loss: 1.8219\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7375 - val_loss: 1.8192\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.7296 - val_loss: 1.8146\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.7234 - val_loss: 1.8075\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7168 - val_loss: 1.8064\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7107 - val_loss: 1.8117\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.7054 - val_loss: 1.8088\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6990 - val_loss: 1.8052\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6945 - val_loss: 1.8044\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.6877 - val_loss: 1.8038\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6831 - val_loss: 1.8076\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 2s 15ms/step - loss: 1.6793 - val_loss: 1.8030\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6727 - val_loss: 1.7990\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6678 - val_loss: 1.8019\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 3s 19ms/step - loss: 1.6637 - val_loss: 1.8009\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6593 - val_loss: 1.7991\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6551 - val_loss: 1.7981\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6516 - val_loss: 1.7968\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6475 - val_loss: 1.7998\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 3s 17ms/step - loss: 1.6436 - val_loss: 1.7957\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6385 - val_loss: 1.7968\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6350 - val_loss: 1.7999\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6317 - val_loss: 1.7997\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 2s 20ms/step - loss: 1.6267 - val_loss: 1.7945\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 3s 20ms/step - loss: 1.6240 - val_loss: 1.7969\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6201 - val_loss: 1.8002\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6164 - val_loss: 1.7951\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.6129 - val_loss: 1.7999\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.6097 - val_loss: 1.8030\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.6062 - val_loss: 1.7989\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 2s 17ms/step - loss: 1.6040 - val_loss: 1.7985\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5997 - val_loss: 1.8012\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5963 - val_loss: 1.8032\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 2s 18ms/step - loss: 1.5948 - val_loss: 1.8056\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 3s 18ms/step - loss: 1.5923 - val_loss: 1.8047\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5889 - val_loss: 1.8030\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5859 - val_loss: 1.8067\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 2s 16ms/step - loss: 1.5832 - val_loss: 1.8061\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.7800\n",
            "eval_loss: 1.779970645904541\n",
            "perplexity 5.929682355574521\n",
            "\n",
            "Докамстлотов плета.\n",
            "Ичедедавомнымы, разво и били сним бу м в гдены,\n",
            "За, нь на ик.\n",
            "\n",
            "Оведнь в лелалем в кой тей ся правеноталеталет стородны,\n",
            "Веза зани иту ве нивень вна лтавзасу вя миль нот, на нышелю пищиводететы вепеньят,\n",
            "И ететазароть сох начегрогами, вамелив ны прам Пы:\n",
            "Ви унерасли лела межезаск,\n",
            "Кой,\n",
            "Ти тсест праходоленый прой бе се лыши плето про вени метаток мелетой года то тогд, м с негл.\n",
            "И ся тывелота дой де нилосто тотреный ный, ся стотано бый итлетотегой гою пой ми зв сяка по счала те прою нуданигопутро нетрора снас и бый ссь,\n",
            "Ве сть.\n",
            "Недестатам пой говемолоденый тисств ви лагрно жим лак ной,\n",
            "Вылить,\n",
            "Одя някав итра погосчаследежалый ужистой моклоротой ждой ни нилю\n",
            "Но,\n",
            "Ветегодна утисока бодо белезах в т в меши –\n",
            "Пужденый,\n",
            "Авежне.\n",
            "Пой заза пый – про во ва ны, мевок во.\n",
            "Праю до ме м х неначта ской\n",
            "Пой леждора лодрегрлелоск за сла вромлой в нежама на берилоре прь посла метожень —\n",
            "Завели мо селодена ебе витозвото жа гота лоданини союбый,\n",
            "\n",
            "«Нось с сть, бой,\n",
            "Моменнно днета то ск ж т\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.768249750137329\n",
            "Epoch 1/70\n",
            "72/72 [==============================] - 10s 75ms/step - loss: 3.4778 - val_loss: 2.7823\n",
            "Epoch 2/70\n",
            "72/72 [==============================] - 3s 36ms/step - loss: 2.6745 - val_loss: 2.5476\n",
            "Epoch 3/70\n",
            "72/72 [==============================] - 4s 44ms/step - loss: 2.5397 - val_loss: 2.4533\n",
            "Epoch 4/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 2.4479 - val_loss: 2.3756\n",
            "Epoch 5/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 2.3767 - val_loss: 2.3220\n",
            "Epoch 6/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 2.3167 - val_loss: 2.2615\n",
            "Epoch 7/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 2.2569 - val_loss: 2.2077\n",
            "Epoch 8/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 2.2030 - val_loss: 2.1550\n",
            "Epoch 9/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 2.1460 - val_loss: 2.1071\n",
            "Epoch 10/70\n",
            "72/72 [==============================] - 3s 30ms/step - loss: 2.0942 - val_loss: 2.0677\n",
            "Epoch 11/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 2.0444 - val_loss: 2.0183\n",
            "Epoch 12/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.9986 - val_loss: 1.9910\n",
            "Epoch 13/70\n",
            "72/72 [==============================] - 3s 26ms/step - loss: 1.9576 - val_loss: 1.9501\n",
            "Epoch 14/70\n",
            "72/72 [==============================] - 3s 26ms/step - loss: 1.9199 - val_loss: 1.9259\n",
            "Epoch 15/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.8864 - val_loss: 1.9066\n",
            "Epoch 16/70\n",
            "72/72 [==============================] - 3s 26ms/step - loss: 1.8562 - val_loss: 1.8853\n",
            "Epoch 17/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.8266 - val_loss: 1.8713\n",
            "Epoch 18/70\n",
            "72/72 [==============================] - 3s 26ms/step - loss: 1.8011 - val_loss: 1.8557\n",
            "Epoch 19/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.7773 - val_loss: 1.8447\n",
            "Epoch 20/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.7550 - val_loss: 1.8364\n",
            "Epoch 21/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.7340 - val_loss: 1.8282\n",
            "Epoch 22/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.7146 - val_loss: 1.8249\n",
            "Epoch 23/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.6964 - val_loss: 1.8181\n",
            "Epoch 24/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.6791 - val_loss: 1.8124\n",
            "Epoch 25/70\n",
            "72/72 [==============================] - 3s 30ms/step - loss: 1.6629 - val_loss: 1.8136\n",
            "Epoch 26/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.6454 - val_loss: 1.8030\n",
            "Epoch 27/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.6274 - val_loss: 1.8061\n",
            "Epoch 28/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.6155 - val_loss: 1.7975\n",
            "Epoch 29/70\n",
            "72/72 [==============================] - 4s 28ms/step - loss: 1.5998 - val_loss: 1.8040\n",
            "Epoch 30/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.5848 - val_loss: 1.8029\n",
            "Epoch 31/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.5702 - val_loss: 1.8035\n",
            "Epoch 32/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.5580 - val_loss: 1.8074\n",
            "Epoch 33/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.5433 - val_loss: 1.8067\n",
            "Epoch 34/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.5301 - val_loss: 1.8094\n",
            "Epoch 35/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.5169 - val_loss: 1.8162\n",
            "Epoch 36/70\n",
            "72/72 [==============================] - 4s 29ms/step - loss: 1.5057 - val_loss: 1.8174\n",
            "Epoch 37/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.4915 - val_loss: 1.8171\n",
            "Epoch 38/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.4800 - val_loss: 1.8265\n",
            "Epoch 39/70\n",
            "72/72 [==============================] - 3s 30ms/step - loss: 1.4676 - val_loss: 1.8312\n",
            "Epoch 40/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.4566 - val_loss: 1.8342\n",
            "Epoch 41/70\n",
            "72/72 [==============================] - 3s 26ms/step - loss: 1.4442 - val_loss: 1.8331\n",
            "Epoch 42/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.4322 - val_loss: 1.8406\n",
            "Epoch 43/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.4224 - val_loss: 1.8474\n",
            "Epoch 44/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.4102 - val_loss: 1.8538\n",
            "Epoch 45/70\n",
            "72/72 [==============================] - 3s 26ms/step - loss: 1.4002 - val_loss: 1.8557\n",
            "Epoch 46/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.3903 - val_loss: 1.8673\n",
            "Epoch 47/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.3782 - val_loss: 1.8714\n",
            "Epoch 48/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.3694 - val_loss: 1.8775\n",
            "Epoch 49/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.3581 - val_loss: 1.8913\n",
            "Epoch 50/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.3482 - val_loss: 1.8945\n",
            "Epoch 51/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.3385 - val_loss: 1.8988\n",
            "Epoch 52/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.3291 - val_loss: 1.9055\n",
            "Epoch 53/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.3197 - val_loss: 1.9151\n",
            "Epoch 54/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.3093 - val_loss: 1.9249\n",
            "Epoch 55/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.3017 - val_loss: 1.9269\n",
            "Epoch 56/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.2935 - val_loss: 1.9331\n",
            "Epoch 57/70\n",
            "72/72 [==============================] - 3s 26ms/step - loss: 1.2847 - val_loss: 1.9453\n",
            "Epoch 58/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.2781 - val_loss: 1.9607\n",
            "Epoch 59/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.2680 - val_loss: 1.9610\n",
            "Epoch 60/70\n",
            "72/72 [==============================] - 3s 26ms/step - loss: 1.2593 - val_loss: 1.9657\n",
            "Epoch 61/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.2525 - val_loss: 1.9793\n",
            "Epoch 62/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.2454 - val_loss: 1.9783\n",
            "Epoch 63/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.2378 - val_loss: 1.9907\n",
            "Epoch 64/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.2285 - val_loss: 1.9915\n",
            "Epoch 65/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.2226 - val_loss: 2.0066\n",
            "Epoch 66/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.2161 - val_loss: 2.0093\n",
            "Epoch 67/70\n",
            "72/72 [==============================] - 3s 28ms/step - loss: 1.2092 - val_loss: 2.0142\n",
            "Epoch 68/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.2045 - val_loss: 2.0225\n",
            "Epoch 69/70\n",
            "72/72 [==============================] - 3s 27ms/step - loss: 1.1963 - val_loss: 2.0295\n",
            "Epoch 70/70\n",
            "72/72 [==============================] - 3s 29ms/step - loss: 1.1896 - val_loss: 2.0335\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.9631\n",
            "eval_loss: 1.9630693197250366\n",
            "perplexity 7.121150644390488\n",
            "\n",
            "Уголасететвь спломежненей новскру дитовецаюбит,\n",
            "Умерала кра бетою м стравеподный, по на те прась ронотелеветыль, ны ст.\n",
            "На номеня!\n",
            "По стаки м вев прето неветои м.\n",
            "Дакаскра нодетиреруказдезотетнололю ний наска спеждадугдеза вобледак нойстоветидафи глеж м,\n",
            "В сх, ная пи и ри\n",
            "Са вольятасх мново беза сли у муто нь,\n",
            "Во, сстисковслеза т вы вобргодушитиреника стрета и сть уте ка прикородласлося не тво ря вихода веде пой горя, укиспродум упе ном бо прокакреный Мастотето убера скастакозгой но по мокорогдерака дла удаколерамитра,\n",
            "Неноскас пратрю воралолуть встанене ты вил.\n",
            "Ся ското дещеблоюто нь\n",
            "Тил ст;\n",
            "За тасто пе, нато ны насмолитола строза и оледетали ст т в и м иля, веть на ной,\n",
            "Оно м по ся метни нь нась,\n",
            "Гегои,\n",
            "И илю м умерогри ут м на.\n",
            "Оносо мирово гогум го водой, в м, м скою с провалеши,\n",
            "И ва сествсетво преги скрезвезта синотроното пи нанот сть, пени т, и, ско и ст,\n",
            "Ося неской, спроне мове стото ужизатоведет м вой\n",
            "Нань ся в встый поре тотежедерася но столь, во бератую на слеголе ра мче </s\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.97491455078125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_gru_units"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "TeDAVKIkME6c",
        "outputId": "33c577d6-14ae-4f00-df48-f2a5a68a7075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              eval_loss perplexity  \\\n",
              "gru_units 10   2.472713   11.85457   \n",
              "gru_units 100  1.938313   6.947024   \n",
              "gru_units 300  1.779971   5.929682   \n",
              "gru_units 500  1.963069   7.121151   \n",
              "\n",
              "                                                     result_text  run_time  \n",
              "gru_units 10   \\nРа ра, бу ут,\\nКой т.\\n\\nНацака ис ся ны дья...  2.813268  \n",
              "gru_units 100  \\nTHiex помпося го\\nБося когл!.\\nБиходнь, сю д...  3.576499  \n",
              "gru_units 300  \\nДокамстлотов плета.\\nИчедедавомнымы, разво и...   2.76825  \n",
              "gru_units 500  \\nУголасететвь спломежненей новскру дитовецаюб...  2.974915  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-541d314c-b6d8-4a2c-a47b-8463a57fcb36\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eval_loss</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>result_text</th>\n",
              "      <th>run_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>gru_units 10</th>\n",
              "      <td>2.472713</td>\n",
              "      <td>11.85457</td>\n",
              "      <td>\\nРа ра, бу ут,\\nКой т.\\n\\nНацака ис ся ны дья...</td>\n",
              "      <td>2.813268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gru_units 100</th>\n",
              "      <td>1.938313</td>\n",
              "      <td>6.947024</td>\n",
              "      <td>\\nTHiex помпося го\\nБося когл!.\\nБиходнь, сю д...</td>\n",
              "      <td>3.576499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gru_units 300</th>\n",
              "      <td>1.779971</td>\n",
              "      <td>5.929682</td>\n",
              "      <td>\\nДокамстлотов плета.\\nИчедедавомнымы, разво и...</td>\n",
              "      <td>2.76825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gru_units 500</th>\n",
              "      <td>1.963069</td>\n",
              "      <td>7.121151</td>\n",
              "      <td>\\nУголасететвь спломежненей новскру дитовецаюб...</td>\n",
              "      <td>2.974915</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-541d314c-b6d8-4a2c-a47b-8463a57fcb36')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-541d314c-b6d8-4a2c-a47b-8463a57fcb36 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-541d314c-b6d8-4a2c-a47b-8463a57fcb36');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in dict_gru_units_describe.items():\n",
        "  print(key, value, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5M47cpAMGqt",
        "outputId": "8a5392eb-9a7f-4019-c5e8-851c5ac30dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gru_units 10\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0       1.000000\n",
            "mean   1000.0   26.0      37.461538\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0   26.0      37.461538\n",
            "25%    1000.0   26.0      37.461538\n",
            "50%    1000.0   26.0      37.461538\n",
            "75%    1000.0   26.0      37.461538\n",
            "max    1000.0   26.0      37.461538\n",
            "gru_units 100\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0       1.000000\n",
            "mean   1000.0   38.0      29.121212\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0   38.0      29.121212\n",
            "25%    1000.0   38.0      29.121212\n",
            "50%    1000.0   38.0      29.121212\n",
            "75%    1000.0   38.0      29.121212\n",
            "max    1000.0   38.0      29.121212\n",
            "gru_units 300\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0          1.000\n",
            "mean   1000.0   25.0         40.625\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0   25.0         40.625\n",
            "25%    1000.0   25.0         40.625\n",
            "50%    1000.0   25.0         40.625\n",
            "75%    1000.0   25.0         40.625\n",
            "max    1000.0   25.0         40.625\n",
            "gru_units 500\n",
            "          len  lines  mean_line_len\n",
            "count     1.0    1.0       1.000000\n",
            "mean   1000.0   18.0      51.684211\n",
            "std       NaN    NaN            NaN\n",
            "min    1000.0   18.0      51.684211\n",
            "25%    1000.0   18.0      51.684211\n",
            "50%    1000.0   18.0      51.684211\n",
            "75%    1000.0   18.0      51.684211\n",
            "max    1000.0   18.0      51.684211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При увеличении числа нейронов перплексия и ошибка на тестовы данных уменьшались, улучшалось качество текстов, время обучения увеличивалось, однако при 500 нейронов результаты стали хуже, что скорее всего свидетельствует о переобучении."
      ],
      "metadata": {
        "id": "s0uVS4zUCrn2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yJl6-VDMDdrN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}