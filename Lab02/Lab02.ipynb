{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №2. Предварительная обработка текстовых данных\n",
    "\n",
    "**Выполнил:** Китайский А.С.\n",
    "\n",
    "**Проверил:** Мохов А.С."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Импорт необходимых модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Загрузка данных"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет «20 news groups» состоит приблизительно из 18000 сообщений на английском языке по 20 тематикам"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Вариант | Класс     |\n",
    "|---------|-----------|\n",
    "| 8       | 5, 16, 20 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| № класса | Название класса          |\n",
    "|----------|--------------------------|\n",
    "| 5        | 'comp.sys.mac.hardware'  |\n",
    "| 16       | 'soc.religion.christian' |\n",
    "| 20       | 'talk.religion.misc'     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['comp.sys.mac.hardware', 'soc.religion.christian', 'talk.religion.misc'] \n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, categories=categories, remove=remove)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories=categories, remove=remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Выведем по одному документа каждого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Документ класса comp.sys.mac.hardware:\n",
      "A SIMM is a small PCB with DRAM chips soldered on.\n",
      "\n",
      "--maarten\n",
      "-------------------------------------------------\n",
      "Документ класса soc.religion.christian:\n",
      "\n",
      "[deletia- and so on]\n",
      "\n",
      "I seem to have been rather unclear.\n",
      "\n",
      "What I was asking is this:\n",
      "\n",
      "Please show me that the most effective substance-absure recovery\n",
      "programs involve meetinsg peoples' spiritual needs, rather than\n",
      "merely attempting to fill peoples' spiritual needs as percieved\n",
      "by the people, A.A, S.R.C. regulars, or snoopy. This will probably\n",
      "involve defining \"spritual needs\" (is it not that clear) and\n",
      "showing that such things exist and how they can be filled.\n",
      "\n",
      "Annother tack you might take is to say that \"fulfilling spiritual\n",
      "needs\" means \"acknowledging a \"higher power\" of some sort, then\n",
      "show that systems that do require this, work better than otherwise\n",
      "identical systems that do not. A correlation here would help you,\n",
      "but as you point out this might just be demonstrating swapping\n",
      "one crutch for annother. (however, I do feel that religion is\n",
      "usually a better crutch than alchohol, as it is not usually\n",
      "poisonous! :) )\n",
      "\n",
      "I hope with that clarification, my question will be answerable. I actually\n",
      "did know about the 12 step program, its the question of what it does,\n",
      "rather than what it tries to do, that makes a difference to me.\n",
      "---\n",
      "\t\t\t- Dan Johnson\n",
      "And God said \"Jeeze, this is dull\"... and it *WAS* dull. Genesis 0:0\n",
      "-------------------------------------------------\n",
      "Документ класса talk.religion.misc:\n",
      "RE: Red, wwhite, and black, the colors of the Imperial German war-flag --\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(categories)):\n",
    "    print(f'Документ класса {categories[i]}:')\n",
    "    print(twenty_train.data[np.where(twenty_train.target == i)[0][0]], \n",
    "          end='\\n-------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество объектов класса comp.sys.mac.hardware: 578\n",
      "Количество объектов класса soc.religion.christian: 599\n",
      "Количество объектов класса talk.religion.misc: 377\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(categories)):\n",
    "    print(f'Количество объектов класса {categories[i]}:', end=' ')\n",
    "    print(np.unique(twenty_train.target, return_counts=True)[1][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы более менее сбалансированы"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Применим стемминг и запишем обработанные выборки в новые переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1554/1554 [00:06<00:00, 249.13it/s]\n",
      "100%|██████████| 1034/1034 [00:04<00:00, 229.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stem_text(text):\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    line = ' '.join([porter_stemmer.stem(word) for word in nltk_tokens])\n",
    "    return line\n",
    "\n",
    "stem_train = list(tqdm(map(stem_text, twenty_train.data), total=len(twenty_train.data)))\n",
    "stem_test = list(tqdm(map(stem_text, twenty_test.data), total=len(twenty_test.data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Проведем векторизацию выборки: a. Векторизуем обучающую и тестовую выборки простым подсчетом слов (CountVectorizer) b. Выведем первые 20 наиболее частотных слов всей выборки и каждого класса по-отдельности. c. Рассчитаем сходство по коэффициенту Жаккара между тремя классами d. Применим процедуру отсечения стоп-слов и повторим пункты b-c. e. Проведем пункты a – c для обучающей и тестовой выборки, для которой проведена процедура стемминга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "mystopwords = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frequency_words(train, class_num=None, stop_words=None):\n",
    "    '''Векторизация выборки и вывод наиболее частотных слов'''\n",
    "    \n",
    "    if class_num is not None:\n",
    "        train = [twenty_train.data[i] for i in np.where(twenty_train.target == class_num)[0]]\n",
    "\n",
    "    vect = CountVectorizer(max_features=10000, stop_words=stop_words)\n",
    "    train_data = vect.fit_transform(train)\n",
    "\n",
    "    x = list(zip(vect.get_feature_names_out(), np.ravel(train_data.sum(axis=0))))\n",
    "    x.sort(key=lambda row: row[1], reverse=True) \n",
    "    if class_num is not None:\n",
    "        print(f'Первые 20 наиболее частотных слов класса {twenty_train.target_names[class_num]}:')\n",
    "    else:\n",
    "        print(f'Первые 20 наиболее частотных слов всей выборки:')\n",
    "    print(x[:20])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./jacc_coef.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_jaccard_coef(first_list, second_list):\n",
    "    '''Рассчет коэффициента Жаккара'''\n",
    "    first_set = set(first_list)\n",
    "    second_set = set(second_list)\n",
    "    total = len(first_set.intersection(second_set))\n",
    "    coef = total / (len(first_set) + len(second_set) - total)\n",
    "    return coef\n",
    "\n",
    "def print_jaccard_coef(data, c0, c1, stop_words=None):\n",
    "    class0 = [data[i] for i in np.where(twenty_train.target == c0)[0]]\n",
    "    class1 = [data[i] for i in np.where(twenty_train.target == c1)[0]]\n",
    "    vect0 = CountVectorizer(max_features=10000, stop_words=stop_words).fit(class0)\n",
    "    vect1 = CountVectorizer(max_features=10000, stop_words=stop_words).fit(class1)\n",
    "    print(f'Коэффициент Жаккара между {c0} и {c1} классами:', end='\\t')\n",
    "    print(round(calc_jaccard_coef(vect0.get_feature_names_out(), vect1.get_feature_names_out()), 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Без стемминга и без отсечения стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('the', 16652), ('to', 8490), ('of', 8334), ('and', 6656), ('that', 5747), ('is', 5591), ('in', 4801), ('it', 3830), ('you', 3092), ('not', 2749), ('for', 2699), ('this', 2486), ('be', 2316), ('are', 2220), ('have', 2166), ('as', 2136), ('with', 2071), ('on', 1823), ('but', 1818), ('was', 1622)]\n",
      "Первые 20 наиболее частотных слов класса comp.sys.mac.hardware:\n",
      "[('the', 3290), ('to', 1544), ('and', 1248), ('of', 972), ('is', 966), ('it', 873), ('in', 748), ('for', 731), ('that', 706), ('with', 622), ('have', 534), ('on', 532), ('you', 531), ('this', 482), ('be', 410), ('if', 402), ('but', 361), ('or', 358), ('can', 357), ('not', 347)]\n",
      "Первые 20 наиболее частотных слов класса soc.religion.christian:\n",
      "[('the', 8723), ('of', 4787), ('to', 4584), ('and', 3422), ('that', 3331), ('is', 3177), ('in', 2707), ('it', 1994), ('not', 1617), ('you', 1499), ('this', 1364), ('be', 1333), ('for', 1317), ('as', 1254), ('are', 1241), ('god', 1097), ('have', 1078), ('he', 1037), ('we', 1030), ('but', 1017)]\n",
      "Первые 20 наиболее частотных слов класса talk.religion.misc:\n",
      "[('the', 4639), ('of', 2575), ('to', 2362), ('and', 1986), ('that', 1710), ('is', 1448), ('in', 1346), ('you', 1062), ('it', 963), ('not', 785), ('for', 651), ('as', 642), ('this', 640), ('are', 633), ('be', 573), ('have', 554), ('with', 508), ('was', 486), ('he', 470), ('they', 445)]\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.198\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.199\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.337\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(twenty_train.data, class_num=None)\n",
    "print_frequency_words(twenty_train.data, class_num=0)\n",
    "print_frequency_words(twenty_train.data, class_num=1)\n",
    "print_frequency_words(twenty_train.data, class_num=2)\n",
    "\n",
    "print_jaccard_coef(twenty_train.data, 0, 1)\n",
    "print_jaccard_coef(twenty_train.data, 0, 2)\n",
    "print_jaccard_coef(twenty_train.data, 1, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее частотными словами для всех классов являются стоп-слова"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Без стемминга и с отсечением стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('god', 1427), ('one', 1140), ('would', 1057), ('people', 779), ('jesus', 722), ('know', 625), ('think', 565), ('like', 559), ('also', 515), ('say', 461), ('may', 456), ('us', 453), ('time', 444), ('believe', 437), ('good', 416), ('church', 414), ('bible', 411), ('even', 410), ('see', 399), ('christian', 396)]\n",
      "Первые 20 наиболее частотных слов класса comp.sys.mac.hardware:\n",
      "[('mac', 327), ('apple', 266), ('one', 246), ('drive', 211), ('would', 200), ('get', 178), ('use', 173), ('problem', 171), ('like', 163), ('know', 162), ('bit', 150), ('scsi', 142), ('system', 139), ('anyone', 123), ('thanks', 120), ('card', 115), ('32', 113), ('memory', 112), ('new', 110), ('also', 109)]\n",
      "Первые 20 наиболее частотных слов класса soc.religion.christian:\n",
      "[('god', 1097), ('would', 610), ('one', 608), ('jesus', 466), ('people', 452), ('church', 340), ('think', 337), ('know', 314), ('us', 304), ('believe', 298), ('christ', 281), ('say', 280), ('also', 277), ('time', 266), ('like', 265), ('faith', 257), ('may', 257), ('bible', 250), ('christians', 242), ('christian', 241)]\n",
      "Первые 20 наиболее частотных слов класса talk.religion.misc:\n",
      "[('god', 329), ('one', 286), ('people', 267), ('jesus', 256), ('would', 247), ('bible', 160), ('christian', 151), ('think', 151), ('know', 149), ('say', 149), ('see', 143), ('even', 140), ('may', 139), ('good', 131), ('like', 131), ('also', 129), ('us', 125), ('many', 122), ('life', 118), ('way', 118)]\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.189\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.191\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.331\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(twenty_train.data, class_num=None, stop_words=mystopwords)\n",
    "print_frequency_words(twenty_train.data, class_num=0, stop_words=mystopwords)\n",
    "print_frequency_words(twenty_train.data, class_num=1, stop_words=mystopwords)\n",
    "print_frequency_words(twenty_train.data, class_num=2, stop_words=mystopwords)\n",
    "\n",
    "print_jaccard_coef(twenty_train.data, 0, 1, stop_words=mystopwords)\n",
    "print_jaccard_coef(twenty_train.data, 0, 2, stop_words=mystopwords)\n",
    "print_jaccard_coef(twenty_train.data, 1, 2, stop_words=mystopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для класса comp.sys.mac.hardware наиболее частотными словами являются слова, связанные с компьютерной техникой, для двух других классов - слова, связанные с религией. Так как последних большинство, то они являются наиболее частотными для всей выборки.\n",
    "\n",
    "Значение коэффициент Жаккара между всеми классами уменьшилось, связано это с тем, что стоп-слов, которые одинаковы для всех классов, теперь нет."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Со стеммингом и с отсечением стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 20 наиболее частотных слов всей выборки:\n",
      "[('thi', 2494), ('wa', 1669), ('god', 1453), ('one', 1193), ('would', 1105), ('hi', 1004), ('christian', 909), ('ha', 867), ('doe', 788), ('peopl', 785), ('say', 759), ('use', 731), ('know', 720), ('jesu', 716), ('think', 659), ('ani', 658), ('onli', 625), ('like', 613), ('believ', 599), ('time', 532)]\n",
      "Первые 20 наиболее частотных слов класса comp.sys.mac.hardware:\n",
      "[('mac', 327), ('apple', 266), ('one', 246), ('drive', 211), ('would', 200), ('get', 178), ('use', 173), ('problem', 171), ('like', 163), ('know', 162), ('bit', 150), ('scsi', 142), ('system', 139), ('anyone', 123), ('thanks', 120), ('card', 115), ('32', 113), ('memory', 112), ('new', 110), ('also', 109)]\n",
      "Первые 20 наиболее частотных слов класса soc.religion.christian:\n",
      "[('god', 1097), ('would', 610), ('one', 608), ('jesus', 466), ('people', 452), ('church', 340), ('think', 337), ('know', 314), ('us', 304), ('believe', 298), ('christ', 281), ('say', 280), ('also', 277), ('time', 266), ('like', 265), ('faith', 257), ('may', 257), ('bible', 250), ('christians', 242), ('christian', 241)]\n",
      "Первые 20 наиболее частотных слов класса talk.religion.misc:\n",
      "[('god', 329), ('one', 286), ('people', 267), ('jesus', 256), ('would', 247), ('bible', 160), ('christian', 151), ('think', 151), ('know', 149), ('say', 149), ('see', 143), ('even', 140), ('may', 139), ('good', 131), ('like', 131), ('also', 129), ('us', 125), ('many', 122), ('life', 118), ('way', 118)]\n",
      "Коэффициент Жаккара между 0 и 1 классами:\t0.205\n",
      "Коэффициент Жаккара между 0 и 2 классами:\t0.214\n",
      "Коэффициент Жаккара между 1 и 2 классами:\t0.347\n"
     ]
    }
   ],
   "source": [
    "print_frequency_words(stem_train, class_num=None, stop_words=mystopwords)\n",
    "print_frequency_words(stem_train, class_num=0, stop_words=mystopwords)\n",
    "print_frequency_words(stem_train, class_num=1, stop_words=mystopwords)\n",
    "print_frequency_words(stem_train, class_num=2, stop_words=mystopwords)\n",
    "\n",
    "print_jaccard_coef(stem_train, 0, 1, stop_words=mystopwords)\n",
    "print_jaccard_coef(stem_train, 0, 2, stop_words=mystopwords)\n",
    "print_jaccard_coef(stem_train, 1, 2, stop_words=mystopwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь для всей выборки наиболее частотными словами являются усеченные слова. За счет этого также увеличилось значение коэффициента Жаррака между всеми классами."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Используя Pipeline, реализуем модель Наивного Байесовского классификатора и выявим на основе показателей качества (значения полноты, точности, f1-меры и аккуратности), какая предварительная обработка данных обеспечит наилучшие результаты классификации."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При проведении данного исследования фиксируем все переменные кроме одной, а далее меняем незафиксированную переменную для определения ее наилучего значения. После того как наилучшее значение найдено, фиксируем это значение, и переходим к следующей переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def pipeline(x_train, y_train, x_test, stop_words=None, max_features=1000, use_tf=True, use_idf=True):\n",
    "    if use_tf:\n",
    "        text_clf = Pipeline([('vect', CountVectorizer(max_features=max_features, stop_words=stop_words)),\n",
    "                             ('tfidf', TfidfTransformer(use_idf=use_idf)),\n",
    "                             ('clf', MultinomialNB()),], \n",
    "                             verbose=10)  \n",
    "    else:\n",
    "        text_clf = Pipeline([('vect', CountVectorizer(max_features=max_features, stop_words=stop_words)),\n",
    "                             ('clf', MultinomialNB()),],\n",
    "                             verbose=10) \n",
    "        \n",
    "    text_clf = text_clf.fit(x_train, y_train)\n",
    "    prediction = text_clf.predict(x_test)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def print_score(prediction, y_test):\n",
    "    '''Расчет показателей качества'''\n",
    "    precision = round(precision_score(prediction, y_test, average='weighted'), 3)\n",
    "    print ('Precision score: ', precision)\n",
    "    recall = round(recall_score(prediction, y_test, average='weighted'), 3)\n",
    "    print ('Recall score: ', recall)\n",
    "    f1 = round(f1_score(prediction, y_test, average='weighted'), 3)\n",
    "    print ('F1-score: ', f1)\n",
    "    accuracy = round(accuracy_score(prediction, y_test), 3)\n",
    "    print ('Accuracy score: ', accuracy)\n",
    "\n",
    "    return [precision, recall, f1, accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['precision', 'recall', 'f1-score', 'accuracy']\n",
    "df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Наличие \\ отсутствие стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.914\n",
      "Recall score:  0.752\n",
      "F1-score:  0.802\n",
      "Accuracy score:  0.752\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(twenty_train.data, twenty_train.target, twenty_test.data)\n",
    "results_withot_stem = print_score(prediction, twenty_test.target)\n",
    "df.loc['results_withot_stem'] = results_withot_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.917\n",
      "Recall score:  0.766\n",
      "F1-score:  0.811\n",
      "Accuracy score:  0.766\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(stem_train, twenty_train.target, stem_test)\n",
    "results_stem = print_score(prediction, twenty_test.target)\n",
    "df.loc['results_stem'] = results_stem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Отсечение \\ не отсечение стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.908\n",
      "Recall score:  0.781\n",
      "F1-score:  0.817\n",
      "Accuracy score:  0.781\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(stem_train, twenty_train.target, stem_test, stop_words=mystopwords)\n",
    "results_stop_words = print_score(prediction, twenty_test.target)\n",
    "df.loc['results_stop_words'] = results_stop_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Взвешивание: Count, TF, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.917\n",
      "Recall score:  0.77\n",
      "F1-score:  0.814\n",
      "Accuracy score:  0.77\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(stem_train, twenty_train.target, stem_test, stop_words=mystopwords, use_tf=True, use_idf=False)\n",
    "results_tf = print_score(prediction, twenty_test.target)\n",
    "df.loc['results_tf'] = results_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 2) Processing vect, total=   0.2s\n",
      "[Pipeline] ............... (step 2 of 2) Processing clf, total=   0.0s\n",
      "Precision score:  0.769\n",
      "Recall score:  0.757\n",
      "F1-score:  0.761\n",
      "Accuracy score:  0.757\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(stem_train, twenty_train.target, stem_test, stop_words=mystopwords, use_tf=False, use_idf=False)\n",
    "results_count = print_score(prediction, twenty_test.target)\n",
    "df.loc['results_count'] = results_stop_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Количество информативных терминов (max_features) - исследовано 5 значений в диапазоне от 100 до общего количества слов в выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.87\n",
      "Recall score:  0.676\n",
      "F1-score:  0.75\n",
      "Accuracy score:  0.676\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.908\n",
      "Recall score:  0.781\n",
      "F1-score:  0.817\n",
      "Accuracy score:  0.781\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.914\n",
      "Recall score:  0.781\n",
      "F1-score:  0.818\n",
      "Accuracy score:  0.781\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.945\n",
      "Recall score:  0.757\n",
      "F1-score:  0.817\n",
      "Accuracy score:  0.757\n",
      "[Pipeline] .............. (step 1 of 3) Processing vect, total=   0.2s\n",
      "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.0s\n",
      "[Pipeline] ............... (step 3 of 3) Processing clf, total=   0.0s\n",
      "Precision score:  0.964\n",
      "Recall score:  0.738\n",
      "F1-score:  0.817\n",
      "Accuracy score:  0.738\n"
     ]
    }
   ],
   "source": [
    "max_featurs = [100, 1000, 2000, 5000, 10000]\n",
    "for i in max_featurs:\n",
    "    prediction = pipeline(stem_train, twenty_train.target, stem_test, stop_words=mystopwords, max_features=i)\n",
    "    results = print_score(prediction, twenty_test.target)\n",
    "    df.loc[f'results_{i}'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>results_withot_stem</th>\n",
       "      <td>0.914</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_stem</th>\n",
       "      <td>0.917</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_stop_words</th>\n",
       "      <td>0.908</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_tf</th>\n",
       "      <td>0.908</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_count</th>\n",
       "      <td>0.908</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_100</th>\n",
       "      <td>0.870</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_1000</th>\n",
       "      <td>0.908</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_2000</th>\n",
       "      <td>0.914</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_5000</th>\n",
       "      <td>0.945</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results_10000</th>\n",
       "      <td>0.964</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     precision  recall  f1-score  accuracy\n",
       "results_withot_stem      0.914   0.752     0.802     0.752\n",
       "results_stem             0.917   0.766     0.811     0.766\n",
       "results_stop_words       0.908   0.781     0.817     0.781\n",
       "results_tf               0.908   0.781     0.817     0.781\n",
       "results_count            0.908   0.781     0.817     0.781\n",
       "results_100              0.870   0.676     0.750     0.676\n",
       "results_1000             0.908   0.781     0.817     0.781\n",
       "results_2000             0.914   0.781     0.818     0.781\n",
       "results_5000             0.945   0.757     0.817     0.757\n",
       "results_10000            0.964   0.738     0.817     0.738"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \n",
    "- Отсечение стоп-слов позволяет избавится от неинформативных признаков;\n",
    "- Стемминг служит для повышения информативности признаков, так как группируются близкие по значению слова, исходя из их общей основы;\n",
    "- Count - самый простой способ векторизации, который заключается в подсчете терминов в документе, TF - частота появления термина в конкретном документе, TF-IDF - позволяет учесть важность термина в документе, а не только частоту его появления;\n",
    "- max_features позволяет ограничить количество информативных признаков\n",
    "\n",
    "Таким образом, наиболее подходящая комбинация параметров следующая: Наличие стемминга, с отсечением стоп слов, TF-IDF векторизация, количество информативных признаков 2000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
